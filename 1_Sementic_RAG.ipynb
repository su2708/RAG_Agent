{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI ë‰´ìŠ¤ Vector DB ìƒì„± ì‹œìŠ¤í…œ \n",
    "\n",
    "AI ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Vector DBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. \n",
    "LangChainê³¼ FAISSë¥¼ í™œìš©í•˜ì—¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì£¼ìš” ê¸°ëŠ¥:\n",
    "1. JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ\n",
    "2. ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í¬ ë¶„í• \n",
    "3. OpenAI Embeddingì„ í†µí•œ ë²¡í„°í™”\n",
    "4. FAISSë¥¼ ì´ìš©í•œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "5. ì§„í–‰ ìƒí™© ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "ì‹œìŠ¤í…œ íŠ¹ì§•:\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬\n",
    "- ì§„í–‰ìƒí™© í‘œì‹œ (tqdm í”„ë¡œê·¸ë ˆìŠ¤ë°”)\n",
    "- ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…\n",
    "- í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•œ ì„¤ì • ê´€ë¦¬\n",
    "\n",
    "ì‹¤í–‰í•˜ê¸° ì „ì— í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜:\n",
    "- OPENAI_API_KEY: OpenAI API í‚¤\n",
    "- OPENAI_EMBEDDING_MODEL: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…\n",
    "- NEWS_FILE_PATH: ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "- VECTOR_STORE_NAME: ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„° DBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "class AINewsRAG:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        \n",
    "        # ë¡œê¹… ì„¤ì •\n",
    "        self.logger = logging.getLogger('AINewsRAG')\n",
    "        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "        # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "    def load_json_files(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"ì—¬ëŸ¬ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        all_documents = []\n",
    "        json_files = glob.glob(f\"{directory_path}/ai_times_news_*.json\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        for file_path in tqdm(json_files, desc=\"JSON íŒŒì¼ ë¡œë“œ ì¤‘\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    documents = json.load(file)\n",
    "                    if documents :\n",
    "                        documents = [doc for doc in documents if len(doc['content']) > 10]\n",
    "                    #ê¸°ì‚¬ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ìƒëµ\n",
    "                    if len(documents) >= 10 : \n",
    "                        all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(all_documents)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return all_documents\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict]) -> List[Document]:\n",
    "        \"\"\"ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\"\"\"\n",
    "        processed_docs = []\n",
    "        self.logger.info(\"ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        for doc in tqdm(documents, desc=\"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘\"):\n",
    "            try:\n",
    "                full_text = f\"ì œëª©: {doc['title']}\\në‚´ìš©: {doc['content']}\"\n",
    "                metadata = {\n",
    "                    'title': doc['title'],\n",
    "                    'url': doc['url'],\n",
    "                    'date': doc['date']\n",
    "                }\n",
    "                \n",
    "                chunks = self.text_splitter.split_text(full_text)\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {doc.get('title', 'Unknown')} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(processed_docs)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        return processed_docs\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "        self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        total_docs = len(documents)\n",
    "        \n",
    "        try:\n",
    "            # ì²­í¬ë¥¼ ë” ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"ë²¡í„° ìƒì„± ì¤‘\"):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                if self.vector_store is None:\n",
    "                    self.vector_store = FAISS.from_documents(batch, self.embeddings)\n",
    "                else:\n",
    "                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)\n",
    "                    self.vector_store.merge_from(batch_vectorstore)\n",
    "            \n",
    "            self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_vector_store(self, path: str):\n",
    "        \"\"\"ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"ë²¡í„° ìŠ¤í† ì–´ë¥¼ {path}ì— ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "            self.vector_store.save_local(path)\n",
    "            self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def load_vector_store(self, path: str):\n",
    "        \"\"\"ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"ë²¡í„° ìŠ¤í† ì–´ë¥¼ {path}ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "            self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization =True)\n",
    "            self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Vector store has not been initialized\")\n",
    "        \n",
    "        self.logger.info(f\"'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        results = self.vector_store.similarity_search(query, k=k)\n",
    "        self.logger.info(f\"{len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16020\\4201626932.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embed_model = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBEDDING_MODEL\"))\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import openai python package. Please install it with `pip install openai`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:330\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m load_dotenv() \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ì„ë² ë”© ëª¨ë¸\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_EMBEDDING_MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\u001b[39;00m\n\u001b[0;32m     10\u001b[0m rag_system \u001b[38;5;241m=\u001b[39m AINewsRAG(embed_model)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[1;34m(values, _)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_core\\utils\\pydantic.py:219\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    216\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:332\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import openai python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install openai`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m     )\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import openai python package. Please install it with `pip install openai`."
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "def main():\n",
    "    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "    load_dotenv() \n",
    "    \n",
    "    # ì„ë² ë”© ëª¨ë¸\n",
    "    embed_model = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBEDDING_MODEL\"))\n",
    "    \n",
    "    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "    rag_system = AINewsRAG(embed_model)\n",
    "    \n",
    "    # JSON íŒŒì¼ë“¤ ë¡œë“œ\n",
    "    documents = rag_system.load_json_files(os.getenv(\"NEWS_FILE_PATH\"))\n",
    "    \n",
    "    # ë¬¸ì„œ ì²˜ë¦¬\n",
    "    processed_docs = rag_system.process_documents(documents)\n",
    "    \n",
    "    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "    rag_system.create_vector_store(processed_docs)\n",
    "    \n",
    "    # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\n",
    "    rag_system.save_vector_store(model=os.getenv(\"VECTOR_STORE_NAME\"))\n",
    "    \n",
    "    # ê²€ìƒ‰ ì˜ˆì‹œ\n",
    "    results = rag_system.search(\"RAGì— ëŒ€í•œ ë‰´ìŠ¤\")\n",
    "    for doc in results:\n",
    "        print(f\"ì œëª©: {doc.metadata['title']}\")\n",
    "        print(f\"ë‚´ìš©: {doc.page_content[:200]}...\")\n",
    "        print(f\"URL: {doc.metadata['url']}\")\n",
    "        print(f\"ë‚ ì§œ: {doc.metadata['date']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:27:41,558 - ë²¡í„° ìŠ¤í† ì–´ë¥¼ ai_news_vectorstoreì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:27:42,130 - ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "'RAG ì‹œìŠ¤í…œ' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:27:46,957 - 'RAG ì‹œìŠ¤í…œ' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:27:47,558 - 5ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ¨ ê²€ìƒ‰ ì™„ë£Œ! 5ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 1/5\n",
      "ì œëª©: ì˜¬ê±°ë‚˜ì´ì¦ˆ,Â RAG ì„±ëŠ¥ í‰ê°€í•˜ëŠ” ë¦¬ë”ë³´ë“œ ê³µê°œ...\"ê¸°ì—…ì˜ RAG ë„ì…ì— ë„ì›€\"\n",
      "ë‚ ì§œ: 2024.05.29 17:40\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=160144\n",
      "ë‚´ìš©:\n",
      "ê¸°ì—…ì´ RAGì„ ë„ì…í•  ê²½ìš° ì–´ë–¤ RAGê°€ ì í•©í•œì§€ ì„±ëŠ¥í‰ê°€ë¥¼ ì œëŒ€ë¡œ í•˜ê¸° ì–´ë µë‹¤ëŠ” ì ì„ ê³ ë ¤, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ë„ ëª¨ë‘ ê³µê°œí–ˆë‹¤. ì‹¤ì œ ì—…ë¬´ ë¬¸ì„œì— í‘œ, ì´ë¯¸ì§€ ë“±ì´ ë³µì¡í•˜ê²Œ ì–½í˜€ ìˆë‹¤ëŠ” ì ì„ ê°ì•ˆí•´ í‘œì™€ ì´ë¯¸ì§€ ë°ì´í„°ë„ ë°ì´í„°ì…‹ì— í¬í•¨ì‹œì¼°ë‹¤.\n",
      "ë¦¬ë”ë³´ë“œëŠ” í—ˆê¹…í˜ì´ìŠ¤ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë¦¬ë”ë³´ë“œì— ë“¤ì–´ê°„ RAG ì†”ë£¨ì…˜ë“¤ë„ ë§í¬ë¡œ ê³µê°œ, ëˆ„êµ¬ë‚˜ ì²´í—˜í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n",
      "ì´ì°½ìˆ˜ ì˜¬ê±°ë‚˜ì´ì¦ˆ ëŒ€í‘œëŠ” \"AI ìƒíƒœê³„ì— ê¸°ì—¬í•˜ê¸° ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ëª¨ë‘ ê³µê°œí–ˆìœ¼ë©°, ì´ë¥¼ í™œìš©í•´ í•œêµ­ì–´ë¡œ ëœ ë‹¤ì–‘í•œ RAG ì†”ë£¨ì…˜ì˜ ì„±ëŠ¥ì´ í–¥ìƒí•˜ê¸¸ ë°”ë€ë‹¤\"ë¼ê³ ...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 2/5\n",
      "ì œëª©: ì¸í…”ë¦¬ì‹œìŠ¤, ë…¸ì½”ë“œ RAG êµ¬ì¶• ì†”ë£¨ì…˜ â€˜ë ˆê·¸ë¹Œë”â€™ ì¶œì‹œ...\"RAG êµ¬ì¶• ì‹œê°„ ì ˆê°\"\n",
      "ë‚ ì§œ: 2024.10.23 12:59\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=164521\n",
      "ë‚´ìš©:\n",
      "ì¸í…”ë¦¬ì‹œìŠ¤, ë…¸ì½”ë“œ RAG êµ¬ì¶• ì†”ë£¨ì…˜ â€˜ë ˆê·¸ë¹Œë”â€™ ì¶œì‹œ...\"RAG êµ¬ì¶• ì‹œê°„ ì ˆê°\"\n",
      " ì¸ê³µì§€ëŠ¥ ì „í™˜(AX) ì „ë¬¸ ì¸í…”ë¦¬ì‹œìŠ¤(ëŒ€í‘œ ë°•ì€ì˜, ì´ìƒêµ¬)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì„ ë…¸ì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ ìë™ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ì†”ë£¨ì…˜ â€˜ë ˆê·¸ë¹Œë”(Rag Builder)â€™ë¥¼ ì¶œì‹œí–ˆë‹¤ê³  23ì¼ ë°í˜”ë‹¤.\n",
      "ëŒ€í˜•ì–¸ì–´ëª¨ë¸(LLM)Â ì„œë¹„ìŠ¤ ê°œë°œì— í•„ìˆ˜ì ì¸ RAGë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” â–²ì •ë³´ìì›ì„ ì§€ì‹ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì²­í‚¹â–² ê° ì²­í¬(ì§€ì‹ ë‹¨ìœ„)ë¥¼ ë²¡í„°í™” í•˜ëŠ” ì„ë² ë”©â–²ì‚¬ìš©ì ì§ˆë¬¸ì— ê·¼ê±°ê°€ ë˜ëŠ” ì²­í¬ë¥¼ ì°¾ì•„ë‚´ëŠ” ì˜ë¯¸ê²€ìƒ‰â–²ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•˜ëŠ” ì¦ê°• ìƒì„±ì— ...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 3/5\n",
      "ì œëª©: \"ë‹¤ì–‘í•œ ì§€ì‹ ì¶œì²˜ ì°¾ì•„ì£¼ëŠ” 'RAG ì—ì´ì „íŠ¸'ê°€ ê²Œì„ ì²´ì¸ì €ë  ê²ƒ\"\n",
      "ë‚ ì§œ: 2024.11.17 19:28\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=165325\n",
      "ë‚´ìš©:\n",
      "RAG íŒŒì´í”„ë¼ì¸ì˜ ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„± ìš”ì†Œì—Â ì ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ì›¹ ê²€ìƒ‰ê³¼,Â ê³„ì‚°ê¸°,Â ë‹¤ì–‘í•œ ì†Œí”„íŠ¸ì›¨ì–´ APIë¥¼ í™œìš©í•´Â ë‹¨ì¼ ì§€ì‹ ì†ŒìŠ¤ ì´ìƒì˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•˜ê³ , ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ë©°, í•„ìš”ì‹œ ì¬ê²€ìƒ‰ì„ í†µí•´ ì •í™•í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•œ ë’¤ ì œë„ˆë ˆì´í„°ë¡œ ì „ë‹¬í•´Â ìµœì¢… ë‹µë³€ì„ ìƒì„±í•œë‹¤.\n",
      "ì´ ë°©ì‹ì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì§€ì‹ ê¸°ë°˜ì„ í™•ì¥ì‹œì¼œ ë³µì¡í•œ ì§ˆë¬¸ì—ë„ ë” ì •í™•í•˜ê³  ê²€ì¦ëœ ì‘ë‹µì„ ì œê³µí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì§€ì› ë‚´ìš©ì´ ì €ì¥ëœ ê²½ìš°, ì‚¬ìš©ìê°€ \"ì˜¤ëŠ˜ ê°€ì¥ ìì£¼ ì œ...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 4/5\n",
      "ì œëª©: ì˜¬ê±°ë‚˜ì´ì¦ˆ,Â RAG ì„±ëŠ¥ í‰ê°€í•˜ëŠ” ë¦¬ë”ë³´ë“œ ê³µê°œ...\"ê¸°ì—…ì˜ RAG ë„ì…ì— ë„ì›€\"\n",
      "ë‚ ì§œ: 2024.05.29 17:40\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=160144\n",
      "ë‚´ìš©:\n",
      "ì˜¬ê±°ë‚˜ì´ì¦ˆ,Â RAG ì„±ëŠ¥ í‰ê°€í•˜ëŠ” ë¦¬ë”ë³´ë“œ ê³µê°œ...\"ê¸°ì—…ì˜ RAG ë„ì…ì— ë„ì›€\"\n",
      " ëŒ€í˜•ì–¸ì–´ëª¨ë¸(LLM) ì „ë¬¸ ì˜¬ê±°ë‚˜ì´ì¦ˆ(ëŒ€í‘œ ì´ì°½ìˆ˜)ëŠ” êµ­ë‚´ ì²˜ìŒìœ¼ë¡œ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” 'ì•Œë¦¬ RAG ë¦¬ë”ë³´ë“œ'ë¥¼ ê³µê°œí•œë‹¤ê³  29ì¼ ë°í˜”ë‹¤.\n",
      "RAG ë¦¬ë”ë³´ë“œëŠ” RAG ê¸°ìˆ ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•´ ìˆœìœ„ë¥¼ ì •í•˜ê³ , ê¸°ì—… AI ì‹¤ë¬´ìë“¤ì´ ê°€ì¥ ì ì ˆí•œ ì†”ë£¨ì…˜ì„ ë¹„êµ ë„ì…í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤ëŠ” ì„¤ëª…ì´ë‹¤.\n",
      "RAGë€ ìƒì„± ì¸ê³µì§€ëŠ¥(AI)ì˜ í™˜ê°ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤. ì‚¬ì „ í•™ìŠµ ë°ì´í„°ë¥¼ ë„˜ì–´ ê¸°ì—… ë‚´ë¶€ ë°ì´í„° ë“± ì´ë¯¸ ì¡´ì¬í•˜ëŠ”...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 5/5\n",
      "ì œëª©: \"RAG ì‚¬ìš©í•´ë„ ë²•ë¥  AI ë„êµ¬ í™˜ê° 33% ì´ìƒ ë°œìƒ\"\n",
      "ë‚ ì§œ: 2024.06.10 18:00\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=160452\n",
      "ë‚´ìš©:\n",
      "\"RAG ì‚¬ìš©í•´ë„ ë²•ë¥  AI ë„êµ¬ í™˜ê° 33% ì´ìƒ ë°œìƒ\"\n",
      " ë²•ë¥  ë¶„ì•¼Â ì¸ê³µì§€ëŠ¥(AI) ë„êµ¬ì˜ ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ê¸°ë²•ì„ ë„ì…í•˜ì§€ë§Œ,Â ì—¬ì „íˆ ì¼ì • ë¹„ìœ¨ë¡œ ë¶€ì •í™•í•œ ì •ë³´ë¥¼ ìƒì„±í•œë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤. ë²•ë¥  ë¶„ì•¼ì—ì„œ AI ë„êµ¬ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí‚¹ê³¼ ê³µê°œ í‰ê°€ì˜ í•„ìš”ì„±ì´ ëŒ€ë‘ëœë‹¤ëŠ” ì§€ì ì´ë‹¤.\n",
      "ë²¤ì²˜ë¹„íŠ¸ëŠ” 7ì¼(í˜„ì§€ì‹œê°„) ìŠ¤íƒ í¬ë“œ ëŒ€í•™ ì—°êµ¬ì§„ì´ ë²•ë¥ AI ë„êµ¬ë“¤ì´ â€˜ì£¼ìš” AI ë²•ë¥  ì—°êµ¬ ë„êµ¬ì˜ ì‹ ë¢°ì„± í‰ê°€(Assessing the Reliability of Leading AI Legal Research Tools)â€™...\n",
      "\n",
      "'AI êµìœ¡' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:27:54,696 - 'AI êµìœ¡' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:27:55,255 - 5ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ¨ ê²€ìƒ‰ ì™„ë£Œ! 5ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 1/5\n",
      "ì œëª©: í•œêµ­AIêµìœ¡í˜‘íšŒ, ìœ ëŸ½ ì¸í„°ë„·ì€í–‰ ì§ì› ëŒ€ìƒ AIêµìœ¡ ì§„í–‰\n",
      "ë‚ ì§œ: 2024.04.25 15:30\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=159121\n",
      "ë‚´ìš©:\n",
      "ë¬¸Â êµìˆ˜ëŠ” ìš°ì²´êµ­ê¸ˆìœµê°œë°œì›ê³¼ í˜‘ë ¥,Â ì´ë²ˆ AIêµìœ¡ê³¼ì •ì„ ì„¤ê³„í•˜ê³  ìš´ì˜ ì±…ì„ì„ ë§¡ì•„ ë§¤ì£¼ êµìœ¡ê³¼ì •ì— ì°¸ì—¬í•˜ì—¬ ê°•ì‚¬ì™€ í”„ë¡œê·¸ë¨ì„ ê´€ë¦¬í•œë‹¤. ìˆ˜ê°•ìƒë“¤ì˜ ë§Œì¡±ë„ëŠ” ë§¤ìš° ë†’ë‹¤ê³  ì „í–ˆë‹¤.\n",
      "ë”ë¶ˆì–´Â ì„±ê· ê´€ëŒ€í•™êµ ìº í¼ìŠ¤ì‚¬ì—…ë‹¨ì€ í•œêµ­AIêµìœ¡í˜‘íšŒÂ·ESGë©”íƒ€ë²„ìŠ¤ë°œì „ì—°êµ¬ì›ê³¼ í˜‘ë ¥,Â ì…ì£¼ ê¸°ì—…ì„ ëŒ€ìƒìœ¼ë¡œÂ AI êµìœ¡ ê³¼ëª©ì„ ê³µë™ ê°œë°œí•´Â 5~6ì›” ì¤‘ì— 8ê°œ ê³¼ëª©ì„ êµìœ¡Â·ë©˜í† ë§ë„ í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ìš´ì˜í•˜ê¸°ë¡œ í–ˆë‹¤. ì„¸ë¶€ ë‚´ìš©ê³¼ ì¼ì •ì€ ê³§ ê³µê°œë  ì˜ˆì •ì´ë‹¤.\n",
      "ê·¸ë¿ë§Œ ì•„ë‹ˆë¼ â€˜AI ESG ìœµí•©ì „ë¬¸ê°€ ì‹¤ì „ê³¼ì •â€™ 1ê¸°ë¥¼ ìˆ™ëª…ì—¬ëŒ€ ë¯¸ë˜êµìœ¡ì›ì— ê°œì„¤í•´Â 9ì¼ë¶€í„° 6ì›”11ì¼ê¹Œì§€ ...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 2/5\n",
      "ì œëª©: ì¤‘êµ­, ëŒ€ì… ì‹œí—˜ ì±„ì ì— AI í…ŒìŠ¤íŠ¸...\"ì¸ê°„ë³´ë‹¤ AIê°€ í•„ê¸°ì²´ ì¸ì‹ ì •í™•\"\n",
      "ë‚ ì§œ: 2024.01.29 18:00\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=156835\n",
      "ë‚´ìš©:\n",
      "ì´ ë°–ì—ë„ ëª¨ì˜ê³ ì‚¬ë¥¼ ì±„ì í•˜ê³  í‹€ë¦° ë¬¸ì œë¥¼ ì‹ë³„í•´ ê°œì¸í™”ëœ í•™ìŠµë²•ì„ ì œê³µí•˜ëŠ” ì†Œë¹„ììš© êµìœ¡ ì•±ë„ ê°œë°œ ì¤‘ì¸ ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤. í˜„ì¬ ìˆ˜ì²œëª…ì„ ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ì´ë©°, ì¤‘êµ­ ì •ë¶€ì˜ ìŠ¹ì¸ì„ ì–»ì€ í›„ ê³µì‹ ì¶œì‹œí•  ì˜ˆì •ì´ë‹¤.\n",
      "ì™• íšŒì¥ì€ â€œì¤‘êµ­ ìµœê³ ì˜ êµìœ¡ ìì›ì€ ë² ì´ì§•, ìƒí•˜ì´, ì„ ì „ ê°™ì€ ê³³ì— ì§‘ì¤‘ë˜ì–´ ìˆì§€ë§Œ ê´‘ëŒ€í•œ ë†ì´Œ ì§€ì—­ì—ëŠ” í›Œë¥­í•œ êµì‚¬ê°€ ë¶€ì¡±í•˜ë‹¤\"ë¼ë©° AIê°€ í•™ìƒê³¼ ë¶€ëª¨ì˜ ë¶€ë‹´ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì´ ìˆë‹¤ê³  ì£¼ì¥í–ˆë‹¤.\n",
      "ì„ëŒ€ì¤€ ê¸°ì ydj@aitimes.com...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 3/5\n",
      "ì œëª©: KT-ê´‘ìš´ì¸ê³µì§€ëŠ¥ê³ , AI êµìœ¡ ì»¤ë¦¬í˜ëŸ¼Â·í”Œë«í¼ êµ¬ì¶• í˜‘ë ¥\n",
      "ë‚ ì§œ: 2022.11.23 13:00\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=148075\n",
      "ë‚´ìš©:\n",
      "KT-ê´‘ìš´ì¸ê³µì§€ëŠ¥ê³ , AI êµìœ¡ ì»¤ë¦¬í˜ëŸ¼Â·í”Œë«í¼ êµ¬ì¶• í˜‘ë ¥\n",
      " KT(ëŒ€í‘œì´ì‚¬ êµ¬í˜„ëª¨)ê°€ ê´‘ìš´ì¸ê³µì§€ëŠ¥ê³ ë“±í•™êµ(í•™êµì¥ ì´ìƒì¢…)ì™€ ì²­ì†Œë…„ AI ì¸ì¬ì–‘ì„±ì„ ìœ„í•œ í˜‘ë ¥ì— ë‚˜ì„ ë‹¤ê³  23ì¼ ë°í˜”ë‹¤.\n",
      "KTì™€ í•™êµ ì¸¡ì€ í•™ìƒë“¤ì˜ AIÂ ì´ë¡  ìŠµë“ê³¼ ë”ë¶ˆì–´ AIë¥¼ ì‹¤ì œ ì ‘ëª©í•  ìˆ˜ ìˆëŠ” í™œìš© ì—­ëŸ‰ì„ í‚¤ìš°ê¸° ìœ„í•´ êµìœ¡ í˜„ì¥ì—ì„œ í™œìš©í•  êµìœ¡ ì»¤ë¦¬í˜ëŸ¼ê³¼ ì½˜í…ì¸ , ì‹¤ìŠµ í”Œë«í¼ ë“±ì„ ë§ˆë ¨í•˜ëŠ” ë° í˜ì„ ëª¨ìœ¼ê¸°ë¡œ í–ˆë‹¤.\n",
      "ê´‘ìš´ì¸ê³µì§€ëŠ¥ê³ ëŠ” 2021ë…„ êµ­ë‚´ì—ì„œ ì²˜ìŒìœ¼ë¡œ AI íŠ¹ì„±í™” ê³ ë“±í•™êµì—Â ì„ ì •ëë‹¤. 2022ë…„ì— ì¸ê³µì§€ëŠ¥ì»´í“¨íŒ…ê³¼, ì¸ê³µì§€ëŠ¥ì „ê¸°ê³¼, ì¸ê³µì§€ëŠ¥ì†Œí”„íŠ¸ì›¨ì–´ 3ê°œ ...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 4/5\n",
      "ì œëª©: KT-ì„œìš¸ì‹œêµìœ¡ì²­, AI ì¸ì¬ì–‘ì„± í˜‘ë ¥...ìê²©ì‹œí—˜ ë“± ì¶”ì§„\n",
      "ë‚ ì§œ: 2023.02.05 09:35\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=149272\n",
      "ë‚´ìš©:\n",
      "KT-ì„œìš¸ì‹œêµìœ¡ì²­, AI ì¸ì¬ì–‘ì„± í˜‘ë ¥...ìê²©ì‹œí—˜ ë“± ì¶”ì§„\n",
      " KT(ëŒ€í‘œ êµ¬í˜„ëª¨)ê°€ ì„œìš¸ì‹œêµìœ¡ì²­ê³¼ ì—…ë¬´í˜‘ì•½ì„ ë§ºê³  íŠ¹ì„±í™”ê³ ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë³¸ê²©ì ì¸ ì²­ì†Œë…„ ì¸ê³µì§€ëŠ¥(AI) ì¸ì¬ì–‘ì„± í˜‘ë ¥ì— ë‚˜ì„ ë‹¤ê³  5ì¼ ë°í˜”ë‹¤. ì„œìš¸ ì§€ì—­ AI ê³ ë“±í•™êµì— â–²AICE(AI í™œìš©ëŠ¥ë ¥ ìê²©ì‹œí—˜) ë„ì… â–²ê³ êµí•™ì ì œ ìš´ì˜ í˜‘ë ¥ â–²êµì› ëŒ€ìƒ AI ì—­ëŸ‰ ê°•í™” ë“±ì„ ì¶”ì§„í•  ì˜ˆì •ì´ë‹¤.\n",
      "ì„œìš¸ì‹œêµìœ¡ì²­ì€ ë¯¸ë˜í˜• ì§ì—…êµìœ¡ ì²´ì œë¥¼ êµ¬ì¶•í•˜ê³  AI ì „ë¬¸ ê¸°ìˆ ì¸ì¬ë¥¼ ì–‘ì„±í•˜ê¸° ìœ„í•´ 2020ë…„ë¶€í„° ì„œìš¸ë””ì§€í…ê³ , ì„ ë¦°ì¸í„°ë„·ê³  ë“±ì„ AI ê³ ë¡œ ì„ ì •í•´ ì§€ì› ì¤‘ì´ë‹¤. í˜„ì¬ ë¯¸ë¦¼ì—¬ìì •ë³´ê³¼í•™ê³ , ì„œ...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 5/5\n",
      "ì œëª©: ê³ ë“±í•™êµ 34ê³³ì—ì„œ ì¸ê³µì§€ëŠ¥ ë°°ìš´ë‹¤...ë‚´ë…„ë¶€í„° ìˆ˜ì—…ì˜ 15% í¸ì„±\n",
      "ë‚ ì§œ: 2020.03.10 15:26\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=126307\n",
      "ë‚´ìš©:\n",
      "ê³ ë“±í•™êµ 34ê³³ì—ì„œ ì¸ê³µì§€ëŠ¥ ë°°ìš´ë‹¤...ë‚´ë…„ë¶€í„° ìˆ˜ì—…ì˜ 15% í¸ì„±\n",
      " êµìœ¡ë¶€ê°€ ì¸ê³µì§€ëŠ¥(AI)ì„ ì „ì²´ êµê³¼ì— 15%ì— í¬í•¨í•œ AI ìœµí•© êµìœ¡ì•ˆì„ ë°œí‘œí–ˆë‹¤.Â 34ê°œ ê±°ì  ê³ ë“±í•™êµì—ì„œ ì‹¤ì‹œí•˜ê³ ,Â ì¸ê·¼ í•™êµì—ì„œë„ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ í•  ê³„íšì´ë‹¤.\n",
      "ì˜¬í•´ëŠ” êµìœ¡ í™˜ê²½ì„ êµ¬ì¶•í•˜ê³ , ë‚´ë…„ ì‹ ì…ìƒ ë¶€í„° 2023ë…„ê¹Œì§€ ìƒˆë¡œìš´ êµìœ¡ê³¼ì •ìœ¼ë¡œ ìš´ì˜í•œë‹¤.\n",
      "ì •ë³´, ì •ë³´ê³¼í•™, í”„ë¡œê·¸ë˜ë°, ë¹…ë°ì´í„° ë¶„ì„, ë°ì´í„° ê³¼í•™, ì¸ê³µì§€ëŠ¥(ê°€ì¹­), ì¸ê³µì§€ëŠ¥ ìˆ˜í•™(ê°€ì¹­) ë“±Â 4ì°¨ ì‚°ì—…í˜ëª… ê´€ë ¨ ë¶„ì•¼ ê³¼ëª©ì„Â ì‹ ì„¤í•œë‹¤.\n",
      "34ê°œ í•™êµëŠ”Â â–³ì„œìš¸ 5ê³³ â–³ê²½ê¸° 5ê³³ â–³ë¶€ì‚° 2ê³³ â–³ëŒ€êµ¬...\n",
      "\n",
      "ğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv() \n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸\n",
    "embed_model = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBEDDING_MODEL\"))\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = AINewsRAG(embed_model)\n",
    "\n",
    "\n",
    "try:\n",
    "    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„\n",
    "    rag.load_vector_store(vector_store_path)\n",
    "    print(\"âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "    \n",
    "\n",
    "#Queryì— ë”°ë¥¸ ë¬¸ì„œ ì°¾ê¸°\n",
    "\n",
    "query = \"AI êµìœ¡\"\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"\\n'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    results = rag.search(query, k=5)\n",
    "    \n",
    "    print(f\"\\nâœ¨ ê²€ìƒ‰ ì™„ë£Œ! {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\\n\")\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ê²€ìƒ‰ ê²°ê³¼ {i}/{len(results)}\")\n",
    "        print(f\"ì œëª©: {doc.metadata['title']}\")\n",
    "        print(f\"ë‚ ì§œ: {doc.metadata['date']}\")\n",
    "        print(f\"{'-'*40}\")\n",
    "        print(f\"URL: {doc.metadata['url']}\")\n",
    "        print(f\"ë‚´ìš©:\\n{doc.page_content[:300]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
    "\n",
    "ì´ ì½”ë“œëŠ” AI ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "1. **ë¬¸ì„œ ì²˜ë¦¬**\n",
    "   - JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ\n",
    "   - ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "   - ë²¡í„° DB ë° í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "2. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**\n",
    "   - ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ (FAISS)\n",
    "   - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25)\n",
    "   - ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í†µí•©\n",
    "\n",
    "3. **ë°ì´í„° ê´€ë¦¬**\n",
    "   - ë²¡í„° ìŠ¤í† ì–´ ì €ì¥/ë¡œë“œ\n",
    "   - ì²˜ë¦¬ëœ ë¬¸ì„œ ë°ì´í„° ì €ì¥/ë¡œë“œ\n",
    "   - ì§„í–‰ ìƒí™© ë¡œê¹…\n",
    "\n",
    "\n",
    "## ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì • ê°€ì´ë“œ\n",
    "\n",
    "- **ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì¤‘ì‹¬ (semantic_weight=0.7)**\n",
    "  - ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ê³ ë ¤\n",
    "  - ìœ ì‚¬í•œ ì£¼ì œì˜ ë¬¸ì„œë„ ê²€ìƒ‰ ê°€ëŠ¥\n",
    "  - ì˜ˆ: \"AI ê¸°ìˆ ì˜ ë¯¸ë˜ ì „ë§\" â†’ AI ë°œì „ ë°©í–¥, ê¸°ìˆ  íŠ¸ë Œë“œ ë“± ê´€ë ¨ ë¬¸ì„œ í¬í•¨\n",
    "\n",
    "- **í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ì‹¬ (semantic_weight=0.3)**\n",
    "  - ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì¤‘ì‹œ\n",
    "  - íŠ¹ì • ìš©ì–´ë‚˜ ê°œë…ì´ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ \n",
    "  - ì˜ˆ: \"ì‚¼ì„±ì „ì AI ì¹©\" â†’ ì •í™•íˆ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ \n",
    "\n",
    "- **ê· í˜•ì¡íŒ ê²€ìƒ‰ (semantic_weight=0.5)**\n",
    "  - ë‘ ë°©ì‹ì˜ ì¥ì ì„ ê· í˜•ìˆê²Œ í™œìš©\n",
    "  - ì¼ë°˜ì ì¸ ê²€ìƒ‰ì— ì í•©\n",
    "  - ì˜ˆ: \"ììœ¨ì£¼í–‰ ì•ˆì „\" â†’ í‚¤ì›Œë“œ ë§¤ì¹­ê³¼ ì˜ë¯¸ì  ì—°ê´€ì„± ëª¨ë‘ ê³ ë ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class AINewsRAG:\n",
    "    \"\"\"\n",
    "    AI ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ\n",
    "    \n",
    "    ì´ í´ë˜ìŠ¤ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë²¡í„° DBë¡œ ë³€í™˜í•˜ê³ , ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„\n",
    "    ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings: OpenAI ì„ë² ë”© ëª¨ë¸\n",
    "        text_splitter: ë¬¸ì„œ ë¶„í• ì„ ìœ„í•œ ìŠ¤í”Œë¦¬í„°\n",
    "        vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ\n",
    "        bm25: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ BM25 ëª¨ë¸\n",
    "        processed_docs: ì²˜ë¦¬ëœ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        doc_mapping: ë¬¸ì„œ IDì™€ ë¬¸ì„œ ê°ì²´ ê°„ì˜ ë§¤í•‘\n",
    "        logger: ë¡œê¹…ì„ ìœ„í•œ ë¡œê±° ê°ì²´\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model):\n",
    "        \"\"\"\n",
    "        AINewsRAG í´ë˜ìŠ¤ ì´ˆê¸°í™”\n",
    "\n",
    "        Args:\n",
    "            embedding_model: OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤\n",
    "        \"\"\"\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self.bm25 = None\n",
    "        self.processed_docs = None\n",
    "        self.doc_mapping = None\n",
    "        \n",
    "        # ë¡œê¹… ì„¤ì •\n",
    "        self.logger = logging.getLogger('AINewsRAG')\n",
    "        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "        # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "    def load_json_files(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ì—¬ëŸ¬ JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: ë¡œë“œëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        Raises:\n",
    "            Exception: íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        json_files = glob.glob(f\"{directory_path}/ai_times_news_*.json\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        for file_path in tqdm(json_files, desc=\"JSON íŒŒì¼ ë¡œë“œ ì¤‘\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    documents = json.load(file)\n",
    "                    if documents:\n",
    "                        documents = [doc for doc in documents if len(doc['content']) > 10]\n",
    "                    if len(documents) >= 10:\n",
    "                        all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(all_documents)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return all_documents\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict]) -> List[Document]:\n",
    "        \"\"\"ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\"\"\"\n",
    "        processed_docs = []\n",
    "        self.logger.info(\"ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        for idx, doc in enumerate(tqdm(documents, desc=\"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘\")):\n",
    "            try:\n",
    "                #ì²« ì •í¬ì— ë¬¸ì„œì˜ ì œëª© í¬í•¨\n",
    "                full_text = f\"{doc['title']}\\n {doc['content']}\"\n",
    "                metadata = {\n",
    "                    'doc_id': idx, \n",
    "                    'title': doc['title'],\n",
    "                    'url': doc['url'],\n",
    "                    'date': doc['date']\n",
    "                }\n",
    "                \n",
    "                chunks = self.text_splitter.split_text(full_text)\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'chunk_id': f\"doc_{idx}_chunk_{chunk_idx}\"  # ì²­í¬ë³„ ê³ ìœ  ID\n",
    "                        }\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {doc.get('title', 'Unknown')} - {str(e)}\")\n",
    "        \n",
    "        self.processed_docs = processed_docs\n",
    "        self.initialize_bm25(processed_docs)\n",
    "        \n",
    "        return processed_docs\n",
    "\n",
    "    def initialize_bm25(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        self.logger.info(\"BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        tokenized_corpus = [\n",
    "            doc.page_content.lower().split() \n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.doc_mapping = {\n",
    "            i: doc for i, doc in enumerate(documents)\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"BM25 ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): ë²¡í„°í™”í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        Raises:\n",
    "            Exception: ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ\n",
    "        \"\"\"\n",
    "        self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        total_docs = len(documents)\n",
    "        \n",
    "        try:\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"ë²¡í„° ìƒì„± ì¤‘\"):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                if self.vector_store is None:\n",
    "                    self.vector_store = FAISS.from_documents(batch, self.embeddings)\n",
    "                else:\n",
    "                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)\n",
    "                    self.vector_store.merge_from(batch_vectorstore)\n",
    "            \n",
    "            self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        í‚¤ì›Œë“œ ê¸°ë°˜ BM25 ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "            k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        Raises:\n",
    "            ValueError: BM25ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°\n",
    "        \"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        self.logger.info(f\"'{query}' í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        top_k_idx = np.argsort(bm25_scores)[-k:][::-1]\n",
    "        results = [\n",
    "            (self.doc_mapping[idx], bm25_scores[idx])\n",
    "            for idx in top_k_idx\n",
    "        ]\n",
    "        \n",
    "        self.logger.info(f\"{len(results)}ê°œì˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(\n",
    "            self, \n",
    "            query: str, \n",
    "            k: int = 5, \n",
    "            semantic_weight: float = 0.5\n",
    "        ) -> List[Tuple[Document, float]]:\n",
    "            \"\"\"\n",
    "            ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "            \"\"\"\n",
    "            self.logger.info(f\"'{query}' í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            semantic_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "            keyword_results = self.keyword_search(query, k=k)\n",
    "            \n",
    "            # ë¬¸ì„œ IDë¥¼ í‚¤ë¡œ ì‚¬ìš©\n",
    "            combined_scores = {}\n",
    "            \n",
    "            # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬\n",
    "            max_semantic_score = max(score for _, score in semantic_results)\n",
    "            for doc, score in semantic_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                \n",
    "                #5ê°œì˜ ë¬¸ì„œì˜ ì ìˆ˜ê°€\n",
    "                normalized_score = 1 - (score / max_semantic_score) \n",
    "                combined_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'score': semantic_weight * normalized_score\n",
    "                }\n",
    "            \n",
    "            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬\n",
    "            max_keyword_score = max(score for _, score in keyword_results)\n",
    "            for doc, score in keyword_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                normalized_score = score / max_keyword_score\n",
    "                if doc_id in combined_scores:\n",
    "                    combined_scores[doc_id]['score'] += (1 - semantic_weight) * normalized_score\n",
    "                else:\n",
    "                    combined_scores[doc_id] = {\n",
    "                        'doc': doc,\n",
    "                        'score': (1 - semantic_weight) * normalized_score\n",
    "                    }\n",
    "            \n",
    "            # ê²°ê³¼ ì •ë ¬\n",
    "            sorted_results = sorted(\n",
    "                [(info['doc'], info['score']) for info in combined_scores.values()],\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "            \n",
    "            self.logger.info(f\"{len(sorted_results)}ê°œì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return sorted_results\n",
    "\n",
    "    def save_vector_store(self, vector_store_path: str, processed_docs_path:str=None):\n",
    "        \"\"\"\n",
    "        ë²¡í„° ìŠ¤í† ì–´ì™€ BM25 ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"ë°ì´í„°ë¥¼ {vector_store_path}ì— ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\n",
    "            os.makedirs(vector_store_path, exist_ok=True)\n",
    "            self.vector_store.save_local(vector_store_path)\n",
    "            \n",
    "            # processed_docs ì €ì¥\n",
    "            if self.processed_docs:\n",
    "                os.makedirs(os.path.dirname(processed_docs_path), exist_ok=True)\n",
    "                with open(processed_docs_path, 'wb') as f:\n",
    "                    pickle.dump(self.processed_docs, f)\n",
    "            \n",
    "            self.logger.info(\"ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_vector_store(self, vector_store_path: str, processed_docs_path):\n",
    "        \"\"\"\n",
    "        ë²¡í„° ìŠ¤í† ì–´ì™€ BM25 ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"ë°ì´í„°ë¥¼ {vector_store_path}ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                vector_store_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # processed_docs ë¡œë“œ\n",
    "            if os.path.exists(processed_docs_path):\n",
    "                with open(processed_docs_path, 'rb') as f:\n",
    "                    self.processed_docs = pickle.load(f)\n",
    "                self.initialize_bm25(self.processed_docs)\n",
    "            \n",
    "            self.logger.info(\"ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\n",
      "2024-11-26 19:41:53,044 - ì´ 85ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JSON íŒŒì¼ ë¡œë“œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 86.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:41:54,040 - ì´ 16973ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "2024-11-26 19:41:54,042 - ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16973/16973 [00:01<00:00, 14529.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:41:55,214 - BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:42:00,913 - BM25 ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "2024-11-26 19:42:01,010 - ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë²¡í„° ìƒì„± ì¤‘:   0%|          | 1/309 [00:06<31:04,  6.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\u001b[39;00m\n\u001b[0;32m     25\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m rag\u001b[38;5;241m.\u001b[39mprocess_documents(documents)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rag\u001b[38;5;241m.\u001b[39msave_vector_store(vector_store_path, processed_doc_path)\n",
      "Cell \u001b[1;32mIn[2], line 171\u001b[0m, in \u001b[0;36mAINewsRAG.create_vector_store\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m         batch_vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mmerge_from(batch_vectorstore)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124më²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_core\\vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m         texts,\n\u001b[0;32m    933\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\resources\\embeddings.py:115\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    107\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 115\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    116\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    117\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    118\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    119\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    120\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    121\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    122\u001b[0m     ),\n\u001b[0;32m    123\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    124\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:84\u001b[0m, in \u001b[0;36mmaybe_transform\u001b[1;34m(data, expected_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:107\u001b[0m, in \u001b[0;36mtransform\u001b[1;34m(data, expected_type)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m     89\u001b[0m     data: _T,\n\u001b[0;32m     90\u001b[0m     expected_type: \u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m     91\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform dictionaries based off of type information from the given type, for example:\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    It should be noted that the transformations that this function does are not represented in the type system.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(_T, transformed)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:168\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    166\u001b[0m stripped_type \u001b[38;5;241m=\u001b[39m strip_annotated_type(inner_type)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_typeddict(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_mapping(data):\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_transform_typeddict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstripped_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:243\u001b[0m, in \u001b[0;36m_transform_typeddict\u001b[1;34m(data, expected_type)\u001b[0m\n\u001b[0;32m    241\u001b[0m         result[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m         result[_maybe_transform_key(key, type_)] \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:185\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n\u001b[1;32m--> 185\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_transform_recursive(d, annotation\u001b[38;5;241m=\u001b[39mannotation, inner_type\u001b[38;5;241m=\u001b[39minner_type) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_transform_recursive(d, annotation\u001b[38;5;241m=\u001b[39mannotation, inner_type\u001b[38;5;241m=\u001b[39minner_type) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:188\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    185\u001b[0m         data \u001b[38;5;241m=\u001b[39m _transform_recursive(data, annotation\u001b[38;5;241m=\u001b[39mannotation, inner_type\u001b[38;5;241m=\u001b[39msubtype)\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseModel\u001b[49m):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_dump(data, exclude_unset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    191\u001b[0m annotated_type \u001b[38;5;241m=\u001b[39m _get_annotated_type(annotation)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\pydantic\\__init__.py:389\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m import_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, package\u001b[38;5;241m=\u001b[39mpackage)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1034\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:222\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[1;34m(name)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:196\u001b[0m, in \u001b[0;36m_get_module_lock\u001b[1;34m(name)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:73\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "print(\"ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ\n",
    "documents = rag.load_json_files(news_dir)\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "processed_docs = rag.process_documents(documents)\n",
    "rag.create_vector_store(processed_docs)\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\n",
    "rag.save_vector_store(vector_store_path, processed_doc_path)\n",
    "print(\"âœ… ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:42:11,832 - ë°ì´í„°ë¥¼ ai_news_vectorstoreì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...\n",
      "2024-11-26 19:42:11,942 - ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: '__fields_set__'\n",
      "ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: '__fields_set__'\n",
      "\n",
      "ğŸ” AI ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "- ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
      "- ê²€ìƒ‰ ë°©ì‹ ë³€ê²½ì€ 'mode [semantic/keyword/hybrid]'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
      "\n",
      "ğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "try:\n",
    "    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„\n",
    "    rag.load_vector_store(vector_store_path, processed_doc_path)\n",
    "    print(\"âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œì‘\n",
    "print(\"\\nğŸ” AI ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "print(\"- ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "print(\"- ê²€ìƒ‰ ë°©ì‹ ë³€ê²½ì€ 'mode [semantic/keyword/hybrid]'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "search_mode = \"hybrid\"\n",
    "while True:\n",
    "    query = input(\"\\nğŸ” ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "        \n",
    "    if query.lower() in ['q', 'quit']:\n",
    "        print(\"\\nğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "        \n",
    "    if query.lower().startswith('mode '):\n",
    "        mode = query.split()[1].lower()\n",
    "        if mode in ['semantic', 'keyword', 'hybrid']:\n",
    "            search_mode = mode\n",
    "            print(f\"\\nâœ… ê²€ìƒ‰ ëª¨ë“œë¥¼ '{mode}'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"\\nâŒ ì˜ëª»ëœ ê²€ìƒ‰ ëª¨ë“œì…ë‹ˆë‹¤. semantic/keyword/hybrid ì¤‘ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ëª¨ë“œ: {search_mode})\")\n",
    "        \n",
    "        if search_mode == \"hybrid\":\n",
    "            results = rag.hybrid_search(query, k=5, semantic_weight=0.7)\n",
    "        elif search_mode == \"semantic\":\n",
    "            results = rag.vector_store.similarity_search_with_score(query, k=5)\n",
    "        else:  # keyword\n",
    "            results = rag.keyword_search(query, k=5)\n",
    "        \n",
    "        print(f\"\\nâœ¨ ê²€ìƒ‰ ì™„ë£Œ! {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\\n\")\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ê²€ìƒ‰ ê²°ê³¼ {i}/{len(results)}\")\n",
    "            print(f\"ì œëª©: {doc.metadata['title']}\")\n",
    "            print(f\"ë‚ ì§œ: {doc.metadata['date']}\")\n",
    "            if search_mode == \"hybrid\":\n",
    "                print(f\"í†µí•© ì ìˆ˜: {score:.4f}\")\n",
    "            elif search_mode == \"semantic\":\n",
    "                print(f\"ìœ ì‚¬ë„ ì ìˆ˜: {1 - (score/2):.4f}\")\n",
    "            else:\n",
    "                print(f\"BM25 ì ìˆ˜: {score:.4f}\")\n",
    "            print(f\"URL: {doc.metadata['url']}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            print(f\"ë‚´ìš©:\\n{doc.page_content[:300]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
