{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 뉴스 Vector DB 생성 시스템 \n",
    "\n",
    "AI 뉴스 데이터를 Vector DB로 변환하여 저장하는 시스템입니다. \n",
    "LangChain과 FAISS를 활용하여 뉴스 데이터를 벡터화하고 효율적으로 검색할 수 있도록 구성되어 있습니다.\n",
    "\n",
    "주요 기능:\n",
    "1. JSON 형식의 뉴스 데이터 로드\n",
    "2. 문서를 적절한 크기로 청크 분할\n",
    "3. OpenAI Embedding을 통한 벡터화\n",
    "4. FAISS를 이용한 벡터 저장소 생성\n",
    "5. 진행 상황 로깅 및 모니터링\n",
    "\n",
    "시스템 특징:\n",
    "- 배치 처리를 통한 대용량 데이터 처리\n",
    "- 진행상황 표시 (tqdm 프로그레스바)\n",
    "- 에러 처리 및 로깅\n",
    "- 환경 변수를 통한 설정 관리\n",
    "\n",
    "실행하기 전에 필요한 환경변수:\n",
    "- OPENAI_API_KEY: OpenAI API 키\n",
    "- OPENAI_EMBEDDING_MODEL: 사용할 임베딩 모델명\n",
    "- NEWS_FILE_PATH: 뉴스 데이터가 있는 디렉토리 경로\n",
    "- VECTOR_STORE_NAME: 벡터 저장소 저장 경로\n",
    "\n",
    "아래 코드를 실행하면 뉴스 데이터를 벡터 DB로 변환하여 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의미 기반 검색 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "class AINewsRAG:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        \n",
    "        # 로깅 설정\n",
    "        self.logger = logging.getLogger('AINewsRAG')\n",
    "        # 기존 핸들러 제거\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "        # 로그 중복 방지\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "    def load_json_files(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"여러 JSON 파일들을 로드합니다.\"\"\"\n",
    "        all_documents = []\n",
    "        json_files = glob.glob(f\"{directory_path}/ai_times_news_*.json\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(json_files)}개의 JSON 파일을 로드합니다...\")\n",
    "        \n",
    "        for file_path in tqdm(json_files, desc=\"JSON 파일 로드 중\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    documents = json.load(file)\n",
    "                    if documents :\n",
    "                        documents = [doc for doc in documents if len(doc['content']) > 10]\n",
    "                    #기사 내용이 없으면 생략\n",
    "                    if len(documents) >= 10 : \n",
    "                        all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"파일 로드 중 오류 발생: {file_path} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(all_documents)}개의 뉴스 기사를 로드했습니다.\")\n",
    "        return all_documents\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict]) -> List[Document]:\n",
    "        \"\"\"문서를 처리하고 청크로 분할합니다.\"\"\"\n",
    "        processed_docs = []\n",
    "        self.logger.info(\"문서 처리 및 청크 분할을 시작합니다...\")\n",
    "        \n",
    "        for doc in tqdm(documents, desc=\"문서 처리 중\"):\n",
    "            try:\n",
    "                full_text = f\"제목: {doc['title']}\\n내용: {doc['content']}\"\n",
    "                metadata = {\n",
    "                    'title': doc['title'],\n",
    "                    'url': doc['url'],\n",
    "                    'date': doc['date']\n",
    "                }\n",
    "                \n",
    "                chunks = self.text_splitter.split_text(full_text)\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"문서 처리 중 오류 발생: {doc.get('title', 'Unknown')} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(processed_docs)}개의 청크가 생성되었습니다.\")\n",
    "        return processed_docs\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"FAISS 벡터 스토어를 생성합니다.\"\"\"\n",
    "        self.logger.info(\"벡터 스토어 생성을 시작합니다...\")\n",
    "        total_docs = len(documents)\n",
    "        \n",
    "        try:\n",
    "            # 청크를 더 작은 배치로 나누어 처리\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"벡터 생성 중\"):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                if self.vector_store is None:\n",
    "                    self.vector_store = FAISS.from_documents(batch, self.embeddings)\n",
    "                else:\n",
    "                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)\n",
    "                    self.vector_store.merge_from(batch_vectorstore)\n",
    "            \n",
    "            self.logger.info(\"벡터 스토어 생성이 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"벡터 스토어 생성 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_vector_store(self, path: str):\n",
    "        \"\"\"벡터 스토어를 로컬에 저장합니다.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"벡터 스토어를 {path}에 저장합니다...\")\n",
    "            self.vector_store.save_local(path)\n",
    "            self.logger.info(\"벡터 스토어 저장이 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"벡터 스토어 저장 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def load_vector_store(self, path: str):\n",
    "        \"\"\"저장된 벡터 스토어를 로드합니다.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"벡터 스토어를 {path}에서 로드합니다...\")\n",
    "            self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization =True)\n",
    "            self.logger.info(\"벡터 스토어 로드가 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"벡터 스토어 로드 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"쿼리와 관련된 문서를 검색합니다.\"\"\"\n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Vector store has not been initialized\")\n",
    "        \n",
    "        self.logger.info(f\"'{query}' 검색을 시작합니다...\")\n",
    "        results = self.vector_store.similarity_search(query, k=k)\n",
    "        self.logger.info(f\"{len(results)}개의 관련 문서를 찾았습니다.\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16020\\4201626932.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embed_model = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBEDDING_MODEL\"))\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import openai python package. Please install it with `pip install openai`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:330\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m load_dotenv() \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 임베딩 모델\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_EMBEDDING_MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# RAG 시스템 초기화\u001b[39;00m\n\u001b[0;32m     10\u001b[0m rag_system \u001b[38;5;241m=\u001b[39m AINewsRAG(embed_model)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[1;34m(values, _)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_core\\utils\\pydantic.py:219\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    216\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\testlangh\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:332\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import openai python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install openai`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m     )\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import openai python package. Please install it with `pip install openai`."
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "def main():\n",
    "    # 환경 변수 로드\n",
    "    load_dotenv() \n",
    "    \n",
    "    # 임베딩 모델\n",
    "    embed_model = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBEDDING_MODEL\"))\n",
    "    \n",
    "    # RAG 시스템 초기화\n",
    "    rag_system = AINewsRAG(embed_model)\n",
    "    \n",
    "    # JSON 파일들 로드\n",
    "    documents = rag_system.load_json_files(os.getenv(\"NEWS_FILE_PATH\"))\n",
    "    \n",
    "    # 문서 처리\n",
    "    processed_docs = rag_system.process_documents(documents)\n",
    "    \n",
    "    # 벡터 스토어 생성\n",
    "    rag_system.create_vector_store(processed_docs)\n",
    "    \n",
    "    # 벡터 스토어 저장\n",
    "    rag_system.save_vector_store(model=os.getenv(\"VECTOR_STORE_NAME\"))\n",
    "    \n",
    "    # 검색 예시\n",
    "    results = rag_system.search(\"RAG에 대한 뉴스\")\n",
    "    for doc in results:\n",
    "        print(f\"제목: {doc.metadata['title']}\")\n",
    "        print(f\"내용: {doc.page_content[:200]}...\")\n",
    "        print(f\"URL: {doc.metadata['url']}\")\n",
    "        print(f\"날짜: {doc.metadata['date']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:27:41,558 - 벡터 스토어를 ai_news_vectorstore에서 로드합니다...\n",
      "2024-11-28 11:27:42,130 - 벡터 스토어 로드가 완료되었습니다.\n",
      "✅ 기존 벡터 스토어를 로드했습니다.\n",
      "\n",
      "'RAG 시스템' 검색을 시작합니다...\n",
      "2024-11-28 11:27:46,957 - 'RAG 시스템' 검색을 시작합니다...\n",
      "2024-11-28 11:27:47,558 - 5개의 관련 문서를 찾았습니다.\n",
      "\n",
      "✨ 검색 완료! 5개의 결과를 찾았습니다.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "검색 결과 1/5\n",
      "제목: 올거나이즈, RAG 성능 평가하는 리더보드 공개...\"기업의 RAG 도입에 도움\"\n",
      "날짜: 2024.05.29 17:40\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=160144\n",
      "내용:\n",
      "기업이 RAG을 도입할 경우 어떤 RAG가 적합한지 성능평가를 제대로 하기 어렵다는 점을 고려, 테스트 데이터셋도 모두 공개했다. 실제 업무 문서에 표, 이미지 등이 복잡하게 얽혀 있다는 점을 감안해 표와 이미지 데이터도 데이터셋에 포함시켰다.\n",
      "리더보드는 허깅페이스를 통해 확인할 수 있다. 리더보드에 들어간 RAG 솔루션들도 링크로 공개, 누구나 체험해 볼 수 있다.\n",
      "이창수 올거나이즈 대표는 \"AI 생태계에 기여하기 위해 테스트 데이터셋을 모두 공개했으며, 이를 활용해 한국어로 된 다양한 RAG 솔루션의 성능이 향상하길 바란다\"라고...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 2/5\n",
      "제목: 인텔리시스, 노코드 RAG 구축 솔루션 ‘레그빌더’ 출시...\"RAG 구축 시간 절감\"\n",
      "날짜: 2024.10.23 12:59\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=164521\n",
      "내용:\n",
      "인텔리시스, 노코드 RAG 구축 솔루션 ‘레그빌더’ 출시...\"RAG 구축 시간 절감\"\n",
      " 인공지능 전환(AX) 전문 인텔리시스(대표 박은영, 이상구)는 검색 증강 생성(RAG)을 노코드 기반으로 자동 구축할 수 있는 솔루션 ‘레그빌더(Rag Builder)’를 출시했다고 23일 밝혔다.\n",
      "대형언어모델(LLM) 서비스 개발에 필수적인 RAG를 적용하기 위해서는 ▲정보자원을 지식 단위로 분할하는 청킹▲ 각 청크(지식 단위)를 벡터화 하는 임베딩▲사용자 질문에 근거가 되는 청크를 찾아내는 의미검색▲정확한 답변을 생성하게 하는 증강 생성에 ...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 3/5\n",
      "제목: \"다양한 지식 출처 찾아주는 'RAG 에이전트'가 게임 체인저될 것\"\n",
      "날짜: 2024.11.17 19:28\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=165325\n",
      "내용:\n",
      "RAG 파이프라인의 리트리버 구성 요소에 적용될 수 있으며, 이를 통해 웹 검색과, 계산기, 다양한 소프트웨어 API를 활용해 단일 지식 소스 이상의 정보를 검색할 수 있다. 사용자의 질문에 맞는 도구를 선택하고, 관련성 있는 정보가 있는지 확인하며, 필요시 재검색을 통해 정확한 정보를 추출한 뒤 제너레이터로 전달해 최종 답변을 생성한다.\n",
      "이 방식은 LLM 애플리케이션의 지식 기반을 확장시켜 복잡한 질문에도 더 정확하고 검증된 응답을 제공한다. 예를 들어, 벡터 데이터베이스에 지원 내용이 저장된 경우, 사용자가 \"오늘 가장 자주 제...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 4/5\n",
      "제목: 올거나이즈, RAG 성능 평가하는 리더보드 공개...\"기업의 RAG 도입에 도움\"\n",
      "날짜: 2024.05.29 17:40\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=160144\n",
      "내용:\n",
      "올거나이즈, RAG 성능 평가하는 리더보드 공개...\"기업의 RAG 도입에 도움\"\n",
      " 대형언어모델(LLM) 전문 올거나이즈(대표 이창수)는 국내 처음으로 검색 증강 생성(RAG) 성능을 측정하는 '알리 RAG 리더보드'를 공개한다고 29일 밝혔다.\n",
      "RAG 리더보드는 RAG 기술의 성능을 측정해 순위를 정하고, 기업 AI 실무자들이 가장 적절한 솔루션을 비교 도입할 수 있도록 지원한다는 설명이다.\n",
      "RAG란 생성 인공지능(AI)의 환각을 최소화할 수 있는 기술로 주목받고 있다. 사전 학습 데이터를 넘어 기업 내부 데이터 등 이미 존재하는...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 5/5\n",
      "제목: \"RAG 사용해도 법률 AI 도구 환각 33% 이상 발생\"\n",
      "날짜: 2024.06.10 18:00\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=160452\n",
      "내용:\n",
      "\"RAG 사용해도 법률 AI 도구 환각 33% 이상 발생\"\n",
      " 법률 분야 인공지능(AI) 도구의 정확성을 높이기 위해 검색 증강 생성(RAG) 기법을 도입하지만, 여전히 일정 비율로 부정확한 정보를 생성한다는 연구 결과가 나왔다. 법률 분야에서 AI 도구에 대한 벤치마킹과 공개 평가의 필요성이 대두된다는 지적이다.\n",
      "벤처비트는 7일(현지시간) 스탠포드 대학 연구진이 법률AI 도구들이 ‘주요 AI 법률 연구 도구의 신뢰성 평가(Assessing the Reliability of Leading AI Legal Research Tools)’...\n",
      "\n",
      "'AI 교육' 검색을 시작합니다...\n",
      "2024-11-28 11:27:54,696 - 'AI 교육' 검색을 시작합니다...\n",
      "2024-11-28 11:27:55,255 - 5개의 관련 문서를 찾았습니다.\n",
      "\n",
      "✨ 검색 완료! 5개의 결과를 찾았습니다.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "검색 결과 1/5\n",
      "제목: 한국AI교육협회, 유럽 인터넷은행 직원 대상 AI교육 진행\n",
      "날짜: 2024.04.25 15:30\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=159121\n",
      "내용:\n",
      "문 교수는 우체국금융개발원과 협력, 이번 AI교육과정을 설계하고 운영 책임을 맡아 매주 교육과정에 참여하여 강사와 프로그램을 관리한다. 수강생들의 만족도는 매우 높다고 전했다.\n",
      "더불어 성균관대학교 캠퍼스사업단은 한국AI교육협회·ESG메타버스발전연구원과 협력, 입주 기업을 대상으로 AI 교육 과목을 공동 개발해 5~6월 중에 8개 과목을 교육·멘토링도 하는 프로그램을 운영하기로 했다. 세부 내용과 일정은 곧 공개될 예정이다.\n",
      "그뿐만 아니라 ‘AI ESG 융합전문가 실전과정’ 1기를 숙명여대 미래교육원에 개설해 9일부터 6월11일까지 ...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 2/5\n",
      "제목: 중국, 대입 시험 채점에 AI 테스트...\"인간보다 AI가 필기체 인식 정확\"\n",
      "날짜: 2024.01.29 18:00\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=156835\n",
      "내용:\n",
      "이 밖에도 모의고사를 채점하고 틀린 문제를 식별해 개인화된 학습법을 제공하는 소비자용 교육 앱도 개발 중인 것으로 알려졌다. 현재 수천명을 대상으로 테스트 중이며, 중국 정부의 승인을 얻은 후 공식 출시할 예정이다.\n",
      "왕 회장은 “중국 최고의 교육 자원은 베이징, 상하이, 선전 같은 곳에 집중되어 있지만 광대한 농촌 지역에는 훌륭한 교사가 부족하다\"라며 AI가 학생과 부모의 부담을 줄일 수 있는 잠재력이 있다고 주장했다.\n",
      "임대준 기자 ydj@aitimes.com...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 3/5\n",
      "제목: KT-광운인공지능고, AI 교육 커리큘럼·플랫폼 구축 협력\n",
      "날짜: 2022.11.23 13:00\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=148075\n",
      "내용:\n",
      "KT-광운인공지능고, AI 교육 커리큘럼·플랫폼 구축 협력\n",
      " KT(대표이사 구현모)가 광운인공지능고등학교(학교장 이상종)와 청소년 AI 인재양성을 위한 협력에 나선다고 23일 밝혔다.\n",
      "KT와 학교 측은 학생들의 AI 이론 습득과 더불어 AI를 실제 접목할 수 있는 활용 역량을 키우기 위해 교육 현장에서 활용할 교육 커리큘럼과 콘텐츠, 실습 플랫폼 등을 마련하는 데 힘을 모으기로 했다.\n",
      "광운인공지능고는 2021년 국내에서 처음으로 AI 특성화 고등학교에 선정됐다. 2022년에 인공지능컴퓨팅과, 인공지능전기과, 인공지능소프트웨어 3개 ...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 4/5\n",
      "제목: KT-서울시교육청, AI 인재양성 협력...자격시험 등 추진\n",
      "날짜: 2023.02.05 09:35\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=149272\n",
      "내용:\n",
      "KT-서울시교육청, AI 인재양성 협력...자격시험 등 추진\n",
      " KT(대표 구현모)가 서울시교육청과 업무협약을 맺고 특성화고를 중심으로 본격적인 청소년 인공지능(AI) 인재양성 협력에 나선다고 5일 밝혔다. 서울 지역 AI 고등학교에 ▲AICE(AI 활용능력 자격시험) 도입 ▲고교학점제 운영 협력 ▲교원 대상 AI 역량 강화 등을 추진할 예정이다.\n",
      "서울시교육청은 미래형 직업교육 체제를 구축하고 AI 전문 기술인재를 양성하기 위해 2020년부터 서울디지텍고, 선린인터넷고 등을 AI 고로 선정해 지원 중이다. 현재 미림여자정보과학고, 서...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 5/5\n",
      "제목: 고등학교 34곳에서 인공지능 배운다...내년부터 수업의 15% 편성\n",
      "날짜: 2020.03.10 15:26\n",
      "----------------------------------------\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=126307\n",
      "내용:\n",
      "고등학교 34곳에서 인공지능 배운다...내년부터 수업의 15% 편성\n",
      " 교육부가 인공지능(AI)을 전체 교과에 15%에 포함한 AI 융합 교육안을 발표했다. 34개 거점 고등학교에서 실시하고, 인근 학교에서도 이용할 수 있도록 할 계획이다.\n",
      "올해는 교육 환경을 구축하고, 내년 신입생 부터 2023년까지 새로운 교육과정으로 운영한다.\n",
      "정보, 정보과학, 프로그래밍, 빅데이터 분석, 데이터 과학, 인공지능(가칭), 인공지능 수학(가칭) 등 4차 산업혁명 관련 분야 과목을 신설한다.\n",
      "34개 학교는 △서울 5곳 △경기 5곳 △부산 2곳 △대구...\n",
      "\n",
      "👋 검색을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "load_dotenv() \n",
    "\n",
    "# 환경 변수에서 경로 가져오기\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "\n",
    "# 임베딩 모델\n",
    "embed_model = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBEDDING_MODEL\"))\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = AINewsRAG(embed_model)\n",
    "\n",
    "\n",
    "try:\n",
    "    # 기존 벡터 스토어 로드 시도\n",
    "    rag.load_vector_store(vector_store_path)\n",
    "    print(\"✅ 기존 벡터 스토어를 로드했습니다.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"벡터 스토어 로드 실패: {str(e)}\")\n",
    "    \n",
    "\n",
    "#Query에 따른 문서 찾기\n",
    "\n",
    "query = \"AI 교육\"\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"\\n'{query}' 검색을 시작합니다...\")\n",
    "    \n",
    "    results = rag.search(query, k=5)\n",
    "    \n",
    "    print(f\"\\n✨ 검색 완료! {len(results)}개의 결과를 찾았습니다.\\n\")\n",
    "    \n",
    "    # 결과 출력\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"검색 결과 {i}/{len(results)}\")\n",
    "        print(f\"제목: {doc.metadata['title']}\")\n",
    "        print(f\"날짜: {doc.metadata['date']}\")\n",
    "        print(f\"{'-'*40}\")\n",
    "        print(f\"URL: {doc.metadata['url']}\")\n",
    "        print(f\"내용:\\n{doc.page_content[:300]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 검색 중 오류가 발생했습니다: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이브리드 검색 시스템\n",
    "\n",
    "이 코드는 AI 관련 뉴스 기사를 효과적으로 검색하기 위한 하이브리드 검색 시스템을 구현한 것입니다.\n",
    "벡터 기반 의미론적 검색과 키워드 기반 검색을 결합하여 더 정확한 검색 결과를 제공합니다.\n",
    "\n",
    "## 주요 기능\n",
    "\n",
    "1. **문서 처리**\n",
    "   - JSON 형식의 뉴스 데이터 로드\n",
    "   - 문서를 청크 단위로 분할\n",
    "   - 벡터 DB 및 키워드 검색용 인덱스 생성\n",
    "\n",
    "2. **하이브리드 검색**\n",
    "   - 벡터 기반 의미론적 검색 (FAISS)\n",
    "   - 키워드 기반 검색 (BM25)\n",
    "   - 두 검색 방식의 결과를 가중치를 적용하여 통합\n",
    "\n",
    "3. **데이터 관리**\n",
    "   - 벡터 스토어 저장/로드\n",
    "   - 처리된 문서 데이터 저장/로드\n",
    "   - 진행 상황 로깅\n",
    "\n",
    "\n",
    "## 검색 가중치 설정 가이드\n",
    "\n",
    "- **의미론적 검색 중심 (semantic_weight=0.7)**\n",
    "  - 문맥과 의미를 더 중요하게 고려\n",
    "  - 유사한 주제의 문서도 검색 가능\n",
    "  - 예: \"AI 기술의 미래 전망\" → AI 발전 방향, 기술 트렌드 등 관련 문서 포함\n",
    "\n",
    "- **키워드 검색 중심 (semantic_weight=0.3)**\n",
    "  - 정확한 키워드 매칭을 중시\n",
    "  - 특정 용어나 개념이 포함된 문서 우선\n",
    "  - 예: \"삼성전자 AI 칩\" → 정확히 해당 키워드가 포함된 문서 우선\n",
    "\n",
    "- **균형잡힌 검색 (semantic_weight=0.5)**\n",
    "  - 두 방식의 장점을 균형있게 활용\n",
    "  - 일반적인 검색에 적합\n",
    "  - 예: \"자율주행 안전\" → 키워드 매칭과 의미적 연관성 모두 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class AINewsRAG:\n",
    "    \"\"\"\n",
    "    AI 뉴스 검색을 위한 RAG(Retrieval-Augmented Generation) 시스템\n",
    "    \n",
    "    이 클래스는 뉴스 기사를 벡터 DB로 변환하고, 의미론적 검색과 키워드 기반 검색을\n",
    "    결합한 하이브리드 검색 기능을 제공합니다.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings: OpenAI 임베딩 모델\n",
    "        text_splitter: 문서 분할을 위한 스플리터\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        bm25: 키워드 기반 검색을 위한 BM25 모델\n",
    "        processed_docs: 처리된 문서들의 리스트\n",
    "        doc_mapping: 문서 ID와 문서 객체 간의 매핑\n",
    "        logger: 로깅을 위한 로거 객체\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model):\n",
    "        \"\"\"\n",
    "        AINewsRAG 클래스 초기화\n",
    "\n",
    "        Args:\n",
    "            embedding_model: OpenAI 임베딩 모델 인스턴스\n",
    "        \"\"\"\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self.bm25 = None\n",
    "        self.processed_docs = None\n",
    "        self.doc_mapping = None\n",
    "        \n",
    "        # 로깅 설정\n",
    "        self.logger = logging.getLogger('AINewsRAG')\n",
    "        # 기존 핸들러 제거\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "        # 로그 중복 방지\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "    def load_json_files(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        여러 JSON 파일에서 뉴스 기사를 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): JSON 파일들이 있는 디렉토리 경로\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 로드된 뉴스 기사 리스트\n",
    "\n",
    "        Raises:\n",
    "            Exception: 파일 로드 중 오류 발생 시\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        json_files = glob.glob(f\"{directory_path}/ai_times_news_*.json\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(json_files)}개의 JSON 파일을 로드합니다...\")\n",
    "        \n",
    "        for file_path in tqdm(json_files, desc=\"JSON 파일 로드 중\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    documents = json.load(file)\n",
    "                    if documents:\n",
    "                        documents = [doc for doc in documents if len(doc['content']) > 10]\n",
    "                    if len(documents) >= 10:\n",
    "                        all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"파일 로드 중 오류 발생: {file_path} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(all_documents)}개의 뉴스 기사를 로드했습니다.\")\n",
    "        return all_documents\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict]) -> List[Document]:\n",
    "        \"\"\"문서를 처리하고 청크로 분할합니다.\"\"\"\n",
    "        processed_docs = []\n",
    "        self.logger.info(\"문서 처리 및 청크 분할을 시작합니다...\")\n",
    "        \n",
    "        for idx, doc in enumerate(tqdm(documents, desc=\"문서 처리 중\")):\n",
    "            try:\n",
    "                #첫 정크에 문서의 제목 포함\n",
    "                full_text = f\"{doc['title']}\\n {doc['content']}\"\n",
    "                metadata = {\n",
    "                    'doc_id': idx, \n",
    "                    'title': doc['title'],\n",
    "                    'url': doc['url'],\n",
    "                    'date': doc['date']\n",
    "                }\n",
    "                \n",
    "                chunks = self.text_splitter.split_text(full_text)\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'chunk_id': f\"doc_{idx}_chunk_{chunk_idx}\"  # 청크별 고유 ID\n",
    "                        }\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"문서 처리 중 오류 발생: {doc.get('title', 'Unknown')} - {str(e)}\")\n",
    "        \n",
    "        self.processed_docs = processed_docs\n",
    "        self.initialize_bm25(processed_docs)\n",
    "        \n",
    "        return processed_docs\n",
    "\n",
    "    def initialize_bm25(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        BM25 검색 엔진을 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): 처리된 문서 리스트\n",
    "        \"\"\"\n",
    "        self.logger.info(\"BM25 검색 엔진을 초기화합니다...\")\n",
    "        \n",
    "        tokenized_corpus = [\n",
    "            doc.page_content.lower().split() \n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.doc_mapping = {\n",
    "            i: doc for i, doc in enumerate(documents)\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"BM25 검색 엔진 초기화가 완료되었습니다.\")\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        FAISS 벡터 스토어를 생성합니다.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): 벡터화할 문서 리스트\n",
    "\n",
    "        Raises:\n",
    "            Exception: 벡터 스토어 생성 중 오류 발생 시\n",
    "        \"\"\"\n",
    "        self.logger.info(\"벡터 스토어 생성을 시작합니다...\")\n",
    "        total_docs = len(documents)\n",
    "        \n",
    "        try:\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"벡터 생성 중\"):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                if self.vector_store is None:\n",
    "                    self.vector_store = FAISS.from_documents(batch, self.embeddings)\n",
    "                else:\n",
    "                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)\n",
    "                    self.vector_store.merge_from(batch_vectorstore)\n",
    "            \n",
    "            self.logger.info(\"벡터 스토어 생성이 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"벡터 스토어 생성 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        키워드 기반 BM25 검색을 수행합니다.\n",
    "\n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            k (int): 반환할 결과 수\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: (문서, 점수) 튜플의 리스트\n",
    "\n",
    "        Raises:\n",
    "            ValueError: BM25가 초기화되지 않은 경우\n",
    "        \"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25가 초기화되지 않았습니다.\")\n",
    "        \n",
    "        self.logger.info(f\"'{query}' 키워드 검색을 시작합니다...\")\n",
    "        \n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        top_k_idx = np.argsort(bm25_scores)[-k:][::-1]\n",
    "        results = [\n",
    "            (self.doc_mapping[idx], bm25_scores[idx])\n",
    "            for idx in top_k_idx\n",
    "        ]\n",
    "        \n",
    "        self.logger.info(f\"{len(results)}개의 키워드 검색 결과를 찾았습니다.\")\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(\n",
    "            self, \n",
    "            query: str, \n",
    "            k: int = 5, \n",
    "            semantic_weight: float = 0.5\n",
    "        ) -> List[Tuple[Document, float]]:\n",
    "            \"\"\"\n",
    "            의미론적 검색과 키워드 검색을 결합한 하이브리드 검색을 수행합니다.\n",
    "            \"\"\"\n",
    "            self.logger.info(f\"'{query}' 하이브리드 검색을 시작합니다...\")\n",
    "            \n",
    "            semantic_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "            keyword_results = self.keyword_search(query, k=k)\n",
    "            \n",
    "            # 문서 ID를 키로 사용\n",
    "            combined_scores = {}\n",
    "            \n",
    "            # 의미론적 검색 결과 처리\n",
    "            max_semantic_score = max(score for _, score in semantic_results)\n",
    "            for doc, score in semantic_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                \n",
    "                #5개의 문서의 점수가\n",
    "                normalized_score = 1 - (score / max_semantic_score) \n",
    "                combined_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'score': semantic_weight * normalized_score\n",
    "                }\n",
    "            \n",
    "            # 키워드 검색 결과 처리\n",
    "            max_keyword_score = max(score for _, score in keyword_results)\n",
    "            for doc, score in keyword_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                normalized_score = score / max_keyword_score\n",
    "                if doc_id in combined_scores:\n",
    "                    combined_scores[doc_id]['score'] += (1 - semantic_weight) * normalized_score\n",
    "                else:\n",
    "                    combined_scores[doc_id] = {\n",
    "                        'doc': doc,\n",
    "                        'score': (1 - semantic_weight) * normalized_score\n",
    "                    }\n",
    "            \n",
    "            # 결과 정렬\n",
    "            sorted_results = sorted(\n",
    "                [(info['doc'], info['score']) for info in combined_scores.values()],\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "            \n",
    "            self.logger.info(f\"{len(sorted_results)}개의 하이브리드 검색 결과를 찾았습니다.\")\n",
    "            return sorted_results\n",
    "\n",
    "    def save_vector_store(self, vector_store_path: str, processed_docs_path:str=None):\n",
    "        \"\"\"\n",
    "        벡터 스토어와 BM25 데이터를 저장합니다.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"데이터를 {vector_store_path}에 저장합니다...\")\n",
    "            \n",
    "            # 벡터 스토어 저장\n",
    "            os.makedirs(vector_store_path, exist_ok=True)\n",
    "            self.vector_store.save_local(vector_store_path)\n",
    "            \n",
    "            # processed_docs 저장\n",
    "            if self.processed_docs:\n",
    "                os.makedirs(os.path.dirname(processed_docs_path), exist_ok=True)\n",
    "                with open(processed_docs_path, 'wb') as f:\n",
    "                    pickle.dump(self.processed_docs, f)\n",
    "            \n",
    "            self.logger.info(\"저장이 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"저장 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_vector_store(self, vector_store_path: str, processed_docs_path):\n",
    "        \"\"\"\n",
    "        벡터 스토어와 BM25 데이터를 로드합니다.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"데이터를 {vector_store_path}에서 로드합니다...\")\n",
    "            \n",
    "            # 벡터 스토어 로드\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                vector_store_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # processed_docs 로드\n",
    "            if os.path.exists(processed_docs_path):\n",
    "                with open(processed_docs_path, 'rb') as f:\n",
    "                    self.processed_docs = pickle.load(f)\n",
    "                self.initialize_bm25(self.processed_docs)\n",
    "            \n",
    "            self.logger.info(\"로드가 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"로드 중 오류 발생: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 스토어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 벡터 스토어를 생성합니다...\n",
      "2024-11-26 19:41:53,044 - 총 85개의 JSON 파일을 로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JSON 파일 로드 중: 100%|██████████| 85/85 [00:00<00:00, 86.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:41:54,040 - 총 16973개의 뉴스 기사를 로드했습니다.\n",
      "2024-11-26 19:41:54,042 - 문서 처리 및 청크 분할을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 처리 중: 100%|██████████| 16973/16973 [00:01<00:00, 14529.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:41:55,214 - BM25 검색 엔진을 초기화합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:42:00,913 - BM25 검색 엔진 초기화가 완료되었습니다.\n",
      "2024-11-26 19:42:01,010 - 벡터 스토어 생성을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "벡터 생성 중:   0%|          | 1/309 [00:06<31:04,  6.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 문서 처리 및 벡터 스토어 생성\u001b[39;00m\n\u001b[0;32m     25\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m rag\u001b[38;5;241m.\u001b[39mprocess_documents(documents)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 벡터 스토어 저장\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rag\u001b[38;5;241m.\u001b[39msave_vector_store(vector_store_path, processed_doc_path)\n",
      "Cell \u001b[1;32mIn[2], line 171\u001b[0m, in \u001b[0;36mAINewsRAG.create_vector_store\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m         batch_vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mmerge_from(batch_vectorstore)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m벡터 스토어 생성이 완료되었습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_core\\vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m         texts,\n\u001b[0;32m    933\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\resources\\embeddings.py:115\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    107\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 115\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    116\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    117\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    118\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    119\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    120\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    121\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    122\u001b[0m     ),\n\u001b[0;32m    123\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    124\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:84\u001b[0m, in \u001b[0;36mmaybe_transform\u001b[1;34m(data, expected_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:107\u001b[0m, in \u001b[0;36mtransform\u001b[1;34m(data, expected_type)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m     89\u001b[0m     data: _T,\n\u001b[0;32m     90\u001b[0m     expected_type: \u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m     91\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform dictionaries based off of type information from the given type, for example:\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    It should be noted that the transformations that this function does are not represented in the type system.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(_T, transformed)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:168\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    166\u001b[0m stripped_type \u001b[38;5;241m=\u001b[39m strip_annotated_type(inner_type)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_typeddict(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_mapping(data):\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_transform_typeddict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstripped_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:243\u001b[0m, in \u001b[0;36m_transform_typeddict\u001b[1;34m(data, expected_type)\u001b[0m\n\u001b[0;32m    241\u001b[0m         result[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m         result[_maybe_transform_key(key, type_)] \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:185\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n\u001b[1;32m--> 185\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_transform_recursive(d, annotation\u001b[38;5;241m=\u001b[39mannotation, inner_type\u001b[38;5;241m=\u001b[39minner_type) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_transform_recursive(d, annotation\u001b[38;5;241m=\u001b[39mannotation, inner_type\u001b[38;5;241m=\u001b[39minner_type) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:177\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# List[T]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     (is_list_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_list(data))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Iterable[T]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (is_iterable_type(stripped_type) \u001b[38;5;129;01mand\u001b[39;00m is_iterable(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     inner_type \u001b[38;5;241m=\u001b[39m extract_type_arg(stripped_type, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_transform_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_union_type(stripped_type):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# For union types we run the transformation against all subtypes to ensure that everything is transformed.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# TODO: there may be edge cases where the same normalized field name will transform to two different names\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# in different subtypes.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subtype \u001b[38;5;129;01min\u001b[39;00m get_args(stripped_type):\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\openai\\_utils\\_transform.py:188\u001b[0m, in \u001b[0;36m_transform_recursive\u001b[1;34m(data, annotation, inner_type)\u001b[0m\n\u001b[0;32m    185\u001b[0m         data \u001b[38;5;241m=\u001b[39m _transform_recursive(data, annotation\u001b[38;5;241m=\u001b[39mannotation, inner_type\u001b[38;5;241m=\u001b[39msubtype)\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseModel\u001b[49m):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_dump(data, exclude_unset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    191\u001b[0m annotated_type \u001b[38;5;241m=\u001b[39m _get_annotated_type(annotation)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\site-packages\\pydantic\\__init__.py:389\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m import_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, package\u001b[38;5;241m=\u001b[39mpackage)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\tjfan\\miniconda3\\envs\\ai_pjt_02\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1034\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:222\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[1;34m(name)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:196\u001b[0m, in \u001b[0;36m_get_module_lock\u001b[1;34m(name)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:73\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# 임베딩 모델 초기화 \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# 환경 변수에서 경로 가져오기\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "print(\"새로운 벡터 스토어를 생성합니다...\")\n",
    "\n",
    "# JSON 파일에서 뉴스 데이터 로드\n",
    "documents = rag.load_json_files(news_dir)\n",
    "\n",
    "# 문서 처리 및 벡터 스토어 생성\n",
    "processed_docs = rag.process_documents(documents)\n",
    "rag.create_vector_store(processed_docs)\n",
    "\n",
    "# 벡터 스토어 저장\n",
    "rag.save_vector_store(vector_store_path, processed_doc_path)\n",
    "print(\"✅ 새로운 벡터 스토어 생성 및 저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이브리드 서치 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-26 19:42:11,832 - 데이터를 ai_news_vectorstore에서 로드합니다...\n",
      "2024-11-26 19:42:11,942 - 로드 중 오류 발생: '__fields_set__'\n",
      "벡터 스토어 로드 실패: '__fields_set__'\n",
      "\n",
      "🔍 AI 뉴스 검색 시스템을 시작합니다.\n",
      "- 종료하려면 'q' 또는 'quit'를 입력하세요.\n",
      "- 검색 방식 변경은 'mode [semantic/keyword/hybrid]'를 입력하세요.\n",
      "\n",
      "👋 검색을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 경로 가져오기\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# 임베딩 모델 초기화 \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "try:\n",
    "    # 기존 벡터 스토어 로드 시도\n",
    "    rag.load_vector_store(vector_store_path, processed_doc_path)\n",
    "    print(\"✅ 기존 벡터 스토어를 로드했습니다.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"벡터 스토어 로드 실패: {str(e)}\")\n",
    "\n",
    "# 대화형 검색 시작\n",
    "print(\"\\n🔍 AI 뉴스 검색 시스템을 시작합니다.\")\n",
    "print(\"- 종료하려면 'q' 또는 'quit'를 입력하세요.\")\n",
    "print(\"- 검색 방식 변경은 'mode [semantic/keyword/hybrid]'를 입력하세요.\")\n",
    "\n",
    "search_mode = \"hybrid\"\n",
    "while True:\n",
    "    query = input(\"\\n🔍 검색할 내용을 입력하세요: \").strip()\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "        \n",
    "    if query.lower() in ['q', 'quit']:\n",
    "        print(\"\\n👋 검색을 종료합니다.\")\n",
    "        break\n",
    "        \n",
    "    if query.lower().startswith('mode '):\n",
    "        mode = query.split()[1].lower()\n",
    "        if mode in ['semantic', 'keyword', 'hybrid']:\n",
    "            search_mode = mode\n",
    "            print(f\"\\n✅ 검색 모드를 '{mode}'로 변경했습니다.\")\n",
    "        else:\n",
    "            print(\"\\n❌ 잘못된 검색 모드입니다. semantic/keyword/hybrid 중 선택하세요.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n'{query}' 검색을 시작합니다... (모드: {search_mode})\")\n",
    "        \n",
    "        if search_mode == \"hybrid\":\n",
    "            results = rag.hybrid_search(query, k=5, semantic_weight=0.7)\n",
    "        elif search_mode == \"semantic\":\n",
    "            results = rag.vector_store.similarity_search_with_score(query, k=5)\n",
    "        else:  # keyword\n",
    "            results = rag.keyword_search(query, k=5)\n",
    "        \n",
    "        print(f\"\\n✨ 검색 완료! {len(results)}개의 결과를 찾았습니다.\\n\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"검색 결과 {i}/{len(results)}\")\n",
    "            print(f\"제목: {doc.metadata['title']}\")\n",
    "            print(f\"날짜: {doc.metadata['date']}\")\n",
    "            if search_mode == \"hybrid\":\n",
    "                print(f\"통합 점수: {score:.4f}\")\n",
    "            elif search_mode == \"semantic\":\n",
    "                print(f\"유사도 점수: {1 - (score/2):.4f}\")\n",
    "            else:\n",
    "                print(f\"BM25 점수: {score:.4f}\")\n",
    "            print(f\"URL: {doc.metadata['url']}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            print(f\"내용:\\n{doc.page_content[:300]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 검색 중 오류가 발생했습니다: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
