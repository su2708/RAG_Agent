import os
import json
import glob
from typing import List, Dict
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import JSONLoader
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from langchain.schema.runnable import RunnablePassthrough
from dotenv import load_dotenv
from tqdm import tqdm
import logging
import sys
import streamlit as st

# ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ 
class AINewsRAG:
    def __init__(self, embedding_model):
        self.embeddings = embedding_model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        self.vector_store = None
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger('AINewsRAG')
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        if self.logger.handlers:
            self.logger.handlers.clear()
        
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        self.logger.addHandler(handler)
        # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€
        self.logger.propagate = False
        
    def load_json_files(self, directory_path: str) -> List[Dict]:
        """ì—¬ëŸ¬ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        all_documents = []
        json_files = glob.glob(f"{directory_path}/ai_times_news_*.json")
        
        self.logger.info(f"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        for file_path in tqdm(json_files, desc="JSON íŒŒì¼ ë¡œë“œ ì¤‘"):
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    documents = json.load(file)
                    if documents :
                        documents = [doc for doc in documents if len(doc['content']) > 10]
                    #ê¸°ì‚¬ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ìƒëµ
                    if len(documents) >= 10 : 
                        all_documents.extend(documents)
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {str(e)}")
        
        self.logger.info(f"ì´ {len(all_documents)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return all_documents
    
    def process_documents(self, documents: List[Dict]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        processed_docs = []
        self.logger.info("ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        for doc in tqdm(documents, desc="ë¬¸ì„œ ì²˜ë¦¬ ì¤‘"):
            try:
                full_text = f"ì œëª©: {doc['title']}\në‚´ìš©: {doc['content']}"
                metadata = {
                    'title': doc['title'],
                    'url': doc['url'],
                    'date': doc['date']
                }
                
                chunks = self.text_splitter.split_text(full_text)
                
                for chunk in chunks:
                    processed_docs.append(Document(
                        page_content=chunk,
                        metadata=metadata
                    ))
            except Exception as e:
                self.logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {doc.get('title', 'Unknown')} - {str(e)}")
        
        self.logger.info(f"ì´ {len(processed_docs)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return processed_docs
    
    def create_vector_store(self, documents: List[Document]):
        """FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        total_docs = len(documents)
        
        try:
            # ì²­í¬ë¥¼ ë” ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            batch_size = 100
            for i in tqdm(range(0, total_docs, batch_size), desc="ë²¡í„° ìƒì„± ì¤‘"):
                batch = documents[i:i+batch_size]
                if self.vector_store is None:
                    self.vector_store = FAISS.from_documents(batch, self.embeddings)
                else:
                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)
                    self.vector_store.merge_from(batch_vectorstore)
            
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def save_vector_store(self, path: str):
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            self.logger.info(f"ë²¡í„° ìŠ¤í† ì–´ë¥¼ {path}ì— ì €ì¥í•©ë‹ˆë‹¤...")
            self.vector_store.save_local(path)
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def load_vector_store(self, path: str):
        """ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            self.logger.info(f"ë²¡í„° ìŠ¤í† ì–´ë¥¼ {path}ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
            self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization =True)
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def search(self, query: str, k: int = 3) -> List[Document]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if self.vector_store is None:
            raise ValueError("Vector store has not been initialized")
        
        self.logger.info(f"'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        results = self.vector_store.similarity_search(query, k=k)
        self.logger.info(f"{len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return results

#text streaming
class StreamHandler(BaseCallbackHandler):
    def __init__ (self, container, initial_text=""):
        self.container = container
        self.text = initial_text
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
def print_messages():
    if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
        for chat_message in st.session_state["messages"]:
            st.chat_message(chat_message.role).write(chat_message.content)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv() 

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
#vector_store_path = "C:/Users/USER/anaconda3/envs/SpartaProjects/Personal_Project/RAG_Agent/RAG_Agent/ai_news_vectorstore/"
vector_store_path = "../ai_news_vectorstore/"

# ì„ë² ë”© ëª¨ë¸
embed_model = OpenAIEmbeddings(model=os.getenv("OPENAI_EMBEDDING_MODEL"))

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag = AINewsRAG(embed_model)

try:
    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
    rag.load_vector_store(vector_store_path)
    print("âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
except Exception as e:
    print(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# Streamlit part ì‹œì‘

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI News",
    page_icon="ğŸ“°",
)

st.title("AI News")

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

with st.sidebar:
    summarize = st.radio(
        "ë‰´ìŠ¤ë¥¼ LLMìœ¼ë¡œ ìš”ì•½í•´ì„œ ì„ë² ë”© í›„ ì €ì¥í• ê¹Œìš”ìš”?",
        (False, True)
    )
    st.session_state["summarize"] = summarize

if summarize:
    pass
else:
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë°›ê¸° 
    if user_input := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ì…ë ¥í•˜ì„¸ìš”."):
        st.chat_message("user").write(user_input)
        st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
        
        # ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = rag.search(user_input, k=5)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        max_context_length = 3000  # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìµœëŒ€ ê¸¸ì´ ì œí•œ
        context = context[:max_context_length]
        
        # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
        print_messages()

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            stream_handler = StreamHandler(st.empty())
            
            # ëª¨ë¸ ìƒì„±
            llm = ChatOpenAI(
                model="gpt-4",
                api_key=OPENAI_API_KEY,
                temperature=0.1,
                streaming=True,
                callbacks=[stream_handler],
                max_tokens=500
            )

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        """
                        contextë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
                        ëª¨ë¥´ë©´ ë‹µë³€ì„ ì§€ì–´ë‚´ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
                        
                        ì•„ë˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì…ë‹ˆë‹¤:\n{context}
                        """
                    ),
                    ("human", "{question}"),
                ]
            )

            chain = {
                "context": lambda x:context,
                "question": RunnablePassthrough(),
            } | prompt | llm

            # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° AI ì‘ë‹µ ìƒì„±
            response = chain.invoke(
                {"question": user_input},
            )
            
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response.content)
            )