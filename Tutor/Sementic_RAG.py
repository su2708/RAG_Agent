#!/usr/bin/env python
# coding: utf-8

# ## AI ë‰´ìŠ¤ Vector DB ìƒì„± ì‹œìŠ¤í…œ 
# 
# AI ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Vector DBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
# LangChainê³¼ FAISSë¥¼ í™œìš©í•˜ì—¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
# 2. ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í¬ ë¶„í• 
# 3. OpenAI Embeddingì„ í†µí•œ ë²¡í„°í™”
# 4. FAISSë¥¼ ì´ìš©í•œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
# 5. ì§„í–‰ ìƒí™© ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
# 
# ì‹œìŠ¤í…œ íŠ¹ì§•:
# - ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
# - ì§„í–‰ìƒí™© í‘œì‹œ (tqdm í”„ë¡œê·¸ë ˆìŠ¤ë°”)
# - ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
# - í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•œ ì„¤ì • ê´€ë¦¬
# 
# ì‹¤í–‰í•˜ê¸° ì „ì— í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜:
# - OPENAI_API_KEY: OpenAI API í‚¤
# - OPENAI_EMBEDDING_MODEL: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
# - NEWS_FILE_PATH: ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
# - VECTOR_STORE_NAME: ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ê²½ë¡œ
# 
# ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„° DBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

# ### ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ

# In[1]:


import os
import json
import glob
from typing import List, Dict
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import JSONLoader
from langchain.schema import Document
from dotenv import load_dotenv
from tqdm import tqdm
import logging
import sys

class AINewsRAG:
    def __init__(self, embedding_model):
        self.embeddings = embedding_model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        self.vector_store = None
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger('AINewsRAG')
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        if self.logger.handlers:
            self.logger.handlers.clear()
        
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        self.logger.addHandler(handler)
        # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€
        self.logger.propagate = False
        
    def load_json_files(self, directory_path: str) -> List[Dict]:
        """ì—¬ëŸ¬ JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        all_documents = []
        json_files = glob.glob(f"{directory_path}/ai_times_news_*.json")
        
        self.logger.info(f"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        for file_path in tqdm(json_files, desc="JSON íŒŒì¼ ë¡œë“œ ì¤‘"):
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    documents = json.load(file)
                    if documents :
                        documents = [doc for doc in documents if len(doc['content']) > 10]
                    #ê¸°ì‚¬ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ìƒëµ
                    if len(documents) >= 10 : 
                        all_documents.extend(documents)
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {str(e)}")
        
        self.logger.info(f"ì´ {len(all_documents)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return all_documents
    
    def process_documents(self, documents: List[Dict]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        processed_docs = []
        self.logger.info("ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        for doc in tqdm(documents, desc="ë¬¸ì„œ ì²˜ë¦¬ ì¤‘"):
            try:
                full_text = f"ì œëª©: {doc['title']}\në‚´ìš©: {doc['content']}"
                metadata = {
                    'title': doc['title'],
                    'url': doc['url'],
                    'date': doc['date']
                }
                
                chunks = self.text_splitter.split_text(full_text)
                
                for chunk in chunks:
                    processed_docs.append(Document(
                        page_content=chunk,
                        metadata=metadata
                    ))
            except Exception as e:
                self.logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {doc.get('title', 'Unknown')} - {str(e)}")
        
        self.logger.info(f"ì´ {len(processed_docs)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return processed_docs
    
    def create_vector_store(self, documents: List[Document]):
        """FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        total_docs = len(documents)
        
        try:
            # ì²­í¬ë¥¼ ë” ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            batch_size = 100
            for i in tqdm(range(0, total_docs, batch_size), desc="ë²¡í„° ìƒì„± ì¤‘"):
                batch = documents[i:i+batch_size]
                if self.vector_store is None:
                    self.vector_store = FAISS.from_documents(batch, self.embeddings)
                else:
                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)
                    self.vector_store.merge_from(batch_vectorstore)
            
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def save_vector_store(self, path: str):
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            self.logger.info(f"ë²¡í„° ìŠ¤í† ì–´ë¥¼ {path}ì— ì €ì¥í•©ë‹ˆë‹¤...")
            self.vector_store.save_local(path)
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def load_vector_store(self, path: str):
        """ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            self.logger.info(f"ë²¡í„° ìŠ¤í† ì–´ë¥¼ {path}ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
            self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization =True)
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def search(self, query: str, k: int = 3) -> List[Document]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if self.vector_store is None:
            raise ValueError("Vector store has not been initialized")
        
        self.logger.info(f"'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        results = self.vector_store.similarity_search(query, k=k)
        self.logger.info(f"{len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return results


# ### Vector DB êµ¬ì¶•

# In[4]:


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv() 
    
    # ì„ë² ë”© ëª¨ë¸
    embed_model = OpenAIEmbeddings(model=os.getenv("OPENAI_EMBEDDING_MODEL"))
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = AINewsRAG(embed_model)
    
    # JSON íŒŒì¼ë“¤ ë¡œë“œ
    documents = rag_system.load_json_files(os.getenv("NEWS_FILE_PATH"))
    
    # ë¬¸ì„œ ì²˜ë¦¬
    processed_docs = rag_system.process_documents(documents)
    
    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    rag_system.create_vector_store(processed_docs)
    
    # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
    rag_system.save_vector_store(model=os.getenv("VECTOR_STORE_NAME"))
    
    # ê²€ìƒ‰ ì˜ˆì‹œ
    results = rag_system.search("RAGì— ëŒ€í•œ ë‰´ìŠ¤")
    for doc in results:
        print(f"ì œëª©: {doc.metadata['title']}")
        print(f"ë‚´ìš©: {doc.page_content[:200]}...")
        print(f"URL: {doc.metadata['url']}")
        print(f"ë‚ ì§œ: {doc.metadata['date']}")
        print("-" * 80)

if __name__ == "__main__":
    main()


# In[ ]:


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv() 

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
vector_store_path = os.getenv("VECTOR_STORE_NAME", "ai_news_vectorstore")

# ì„ë² ë”© ëª¨ë¸
embed_model = OpenAIEmbeddings(model=os.getenv("OPENAI_EMBEDDING_MODEL"))

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag = AINewsRAG(embed_model)


try:
    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
    rag.load_vector_store(vector_store_path)
    print("âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
except Exception as e:
    print(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    

#Queryì— ë”°ë¥¸ ë¬¸ì„œ ì°¾ê¸°

query = "AI êµìœ¡"


try:
    print(f"\n'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    results = rag.search(query, k=5)
    
    print(f"\nâœ¨ ê²€ìƒ‰ ì™„ë£Œ! {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    # ê²°ê³¼ ì¶œë ¥
    for i, doc in enumerate(results, 1):
        print(f"\n{'='*80}")
        print(f"ê²€ìƒ‰ ê²°ê³¼ {i}/{len(results)}")
        print(f"ì œëª©: {doc.metadata['title']}")
        print(f"ë‚ ì§œ: {doc.metadata['date']}")
        print(f"{'-'*40}")
        print(f"URL: {doc.metadata['url']}")
        print(f"ë‚´ìš©:\n{doc.page_content[:300]}...")
        
except Exception as e:
    print(f"\nâŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


# # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
# 
# ì´ ì½”ë“œëŠ” AI ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.
# ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
# 
# ## ì£¼ìš” ê¸°ëŠ¥
# 
# 1. **ë¬¸ì„œ ì²˜ë¦¬**
#    - JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
#    - ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
#    - ë²¡í„° DB ë° í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ ìƒì„±
# 
# 2. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**
#    - ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ (FAISS)
#    - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25)
#    - ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í†µí•©
# 
# 3. **ë°ì´í„° ê´€ë¦¬**
#    - ë²¡í„° ìŠ¤í† ì–´ ì €ì¥/ë¡œë“œ
#    - ì²˜ë¦¬ëœ ë¬¸ì„œ ë°ì´í„° ì €ì¥/ë¡œë“œ
#    - ì§„í–‰ ìƒí™© ë¡œê¹…
# 
# 
# ## ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì • ê°€ì´ë“œ
# 
# - **ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì¤‘ì‹¬ (semantic_weight=0.7)**
#   - ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ê³ ë ¤
#   - ìœ ì‚¬í•œ ì£¼ì œì˜ ë¬¸ì„œë„ ê²€ìƒ‰ ê°€ëŠ¥
#   - ì˜ˆ: "AI ê¸°ìˆ ì˜ ë¯¸ë˜ ì „ë§" â†’ AI ë°œì „ ë°©í–¥, ê¸°ìˆ  íŠ¸ë Œë“œ ë“± ê´€ë ¨ ë¬¸ì„œ í¬í•¨
# 
# - **í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ì‹¬ (semantic_weight=0.3)**
#   - ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì¤‘ì‹œ
#   - íŠ¹ì • ìš©ì–´ë‚˜ ê°œë…ì´ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ 
#   - ì˜ˆ: "ì‚¼ì„±ì „ì AI ì¹©" â†’ ì •í™•íˆ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ 
# 
# - **ê· í˜•ì¡íŒ ê²€ìƒ‰ (semantic_weight=0.5)**
#   - ë‘ ë°©ì‹ì˜ ì¥ì ì„ ê· í˜•ìˆê²Œ í™œìš©
#   - ì¼ë°˜ì ì¸ ê²€ìƒ‰ì— ì í•©
#   - ì˜ˆ: "ììœ¨ì£¼í–‰ ì•ˆì „" â†’ í‚¤ì›Œë“œ ë§¤ì¹­ê³¼ ì˜ë¯¸ì  ì—°ê´€ì„± ëª¨ë‘ ê³ ë ¤

# In[ ]:


import os
import json
import glob
import pickle
import logging
import sys
import numpy as np
from typing import List, Dict, Tuple
from tqdm import tqdm
from rank_bm25 import BM25Okapi
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

class AINewsRAG:
    """
    AI ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ
    
    ì´ í´ë˜ìŠ¤ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë²¡í„° DBë¡œ ë³€í™˜í•˜ê³ , ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„
    ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

    Attributes:
        embeddings: OpenAI ì„ë² ë”© ëª¨ë¸
        text_splitter: ë¬¸ì„œ ë¶„í• ì„ ìœ„í•œ ìŠ¤í”Œë¦¬í„°
        vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ
        bm25: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ BM25 ëª¨ë¸
        processed_docs: ì²˜ë¦¬ëœ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        doc_mapping: ë¬¸ì„œ IDì™€ ë¬¸ì„œ ê°ì²´ ê°„ì˜ ë§¤í•‘
        logger: ë¡œê¹…ì„ ìœ„í•œ ë¡œê±° ê°ì²´
    """

    def __init__(self, embedding_model):
        """
        AINewsRAG í´ë˜ìŠ¤ ì´ˆê¸°í™”

        Args:
            embedding_model: OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        self.embeddings = embedding_model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        self.vector_store = None
        self.bm25 = None
        self.processed_docs = None
        self.doc_mapping = None
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger('AINewsRAG')
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        if self.logger.handlers:
            self.logger.handlers.clear()
        
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        self.logger.addHandler(handler)
        # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€
        self.logger.propagate = False
        
    def load_json_files(self, directory_path: str) -> List[Dict]:
        """
        ì—¬ëŸ¬ JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            directory_path (str): JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ

        Returns:
            List[Dict]: ë¡œë“œëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸

        Raises:
            Exception: íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        all_documents = []
        json_files = glob.glob(f"{directory_path}/ai_times_news_*.json")
        
        self.logger.info(f"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        for file_path in tqdm(json_files, desc="JSON íŒŒì¼ ë¡œë“œ ì¤‘"):
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    documents = json.load(file)
                    if documents:
                        documents = [doc for doc in documents if len(doc['content']) > 10]
                    if len(documents) >= 10:
                        all_documents.extend(documents)
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {str(e)}")
        
        self.logger.info(f"ì´ {len(all_documents)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return all_documents
    
    def process_documents(self, documents: List[Dict]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        processed_docs = []
        self.logger.info("ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        for idx, doc in enumerate(tqdm(documents, desc="ë¬¸ì„œ ì²˜ë¦¬ ì¤‘")):
            try:
                #ì²« ì •í¬ì— ë¬¸ì„œì˜ ì œëª© í¬í•¨
                full_text = f"{doc['title']}\n {doc['content']}"
                metadata = {
                    'doc_id': idx, 
                    'title': doc['title'],
                    'url': doc['url'],
                    'date': doc['date']
                }
                
                chunks = self.text_splitter.split_text(full_text)
                
                for chunk_idx, chunk in enumerate(chunks):
                    processed_docs.append(Document(
                        page_content=chunk,
                        metadata={
                            **metadata,
                            'chunk_id': f"doc_{idx}_chunk_{chunk_idx}"  # ì²­í¬ë³„ ê³ ìœ  ID
                        }
                    ))
            except Exception as e:
                self.logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {doc.get('title', 'Unknown')} - {str(e)}")
        
        self.processed_docs = processed_docs
        self.initialize_bm25(processed_docs)
        
        return processed_docs

    def initialize_bm25(self, documents: List[Document]):
        """
        BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            documents (List[Document]): ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        self.logger.info("BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        
        tokenized_corpus = [
            doc.page_content.lower().split() 
            for doc in documents
        ]
        
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.doc_mapping = {
            i: doc for i, doc in enumerate(documents)
        }
        
        self.logger.info("BM25 ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def create_vector_store(self, documents: List[Document]):
        """
        FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            documents (List[Document]): ë²¡í„°í™”í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Raises:
            Exception: ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        total_docs = len(documents)
        
        try:
            batch_size = 100
            for i in tqdm(range(0, total_docs, batch_size), desc="ë²¡í„° ìƒì„± ì¤‘"):
                batch = documents[i:i+batch_size]
                if self.vector_store is None:
                    self.vector_store = FAISS.from_documents(batch, self.embeddings)
                else:
                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)
                    self.vector_store.merge_from(batch_vectorstore)
            
            self.logger.info("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ BM25 ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            List[Tuple[Document, float]]: (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸

        Raises:
            ValueError: BM25ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        """
        if self.bm25 is None:
            raise ValueError("BM25ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.logger.info(f"'{query}' í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        tokenized_query = query.lower().split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        top_k_idx = np.argsort(bm25_scores)[-k:][::-1]
        results = [
            (self.doc_mapping[idx], bm25_scores[idx])
            for idx in top_k_idx
        ]
        
        self.logger.info(f"{len(results)}ê°œì˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return results

    def hybrid_search(
            self, 
            query: str, 
            k: int = 5, 
            semantic_weight: float = 0.5
        ) -> List[Tuple[Document, float]]:
            """
            ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            """
            self.logger.info(f"'{query}' í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            semantic_results = self.vector_store.similarity_search_with_score(query, k=k)
            keyword_results = self.keyword_search(query, k=k)
            
            # ë¬¸ì„œ IDë¥¼ í‚¤ë¡œ ì‚¬ìš©
            combined_scores = {}
            
            # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            max_semantic_score = max(score for _, score in semantic_results)
            for doc, score in semantic_results:
                doc_id = doc.metadata['chunk_id']
                
                #5ê°œì˜ ë¬¸ì„œì˜ ì ìˆ˜ê°€
                normalized_score = 1 - (score / max_semantic_score) 
                combined_scores[doc_id] = {
                    'doc': doc,
                    'score': semantic_weight * normalized_score
                }
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            max_keyword_score = max(score for _, score in keyword_results)
            for doc, score in keyword_results:
                doc_id = doc.metadata['chunk_id']
                normalized_score = score / max_keyword_score
                if doc_id in combined_scores:
                    combined_scores[doc_id]['score'] += (1 - semantic_weight) * normalized_score
                else:
                    combined_scores[doc_id] = {
                        'doc': doc,
                        'score': (1 - semantic_weight) * normalized_score
                    }
            
            # ê²°ê³¼ ì •ë ¬
            sorted_results = sorted(
                [(info['doc'], info['score']) for info in combined_scores.values()],
                key=lambda x: x[1],
                reverse=True
            )[:k]
            
            self.logger.info(f"{len(sorted_results)}ê°œì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return sorted_results

    def save_vector_store(self, vector_store_path: str, processed_docs_path:str=None):
        """
        ë²¡í„° ìŠ¤í† ì–´ì™€ BM25 ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        try:
            self.logger.info(f"ë°ì´í„°ë¥¼ {vector_store_path}ì— ì €ì¥í•©ë‹ˆë‹¤...")
            
            # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
            os.makedirs(vector_store_path, exist_ok=True)
            self.vector_store.save_local(vector_store_path)
            
            # processed_docs ì €ì¥
            if self.processed_docs:
                os.makedirs(os.path.dirname(processed_docs_path), exist_ok=True)
                with open(processed_docs_path, 'wb') as f:
                    pickle.dump(self.processed_docs, f)
            
            self.logger.info("ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    def load_vector_store(self, vector_store_path: str, processed_docs_path):
        """
        ë²¡í„° ìŠ¤í† ì–´ì™€ BM25 ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        try:
            self.logger.info(f"ë°ì´í„°ë¥¼ {vector_store_path}ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
            self.vector_store = FAISS.load_local(
                vector_store_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # processed_docs ë¡œë“œ
            if os.path.exists(processed_docs_path):
                with open(processed_docs_path, 'rb') as f:
                    self.processed_docs = pickle.load(f)
                self.initialize_bm25(self.processed_docs)
            
            self.logger.info("ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise


# ### ë²¡í„° ìŠ¤í† ì–´ ìƒì„±

# In[3]:


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
import os
load_dotenv()

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” 
embedding_model = OpenAIEmbeddings(
    model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
vector_store_path = os.getenv("VECTOR_STORE_NAME", "ai_news_vectorstore")
news_dir = os.getenv("NEWS_FILE_PATH", "./ai_news")
processed_doc_path = os.getenv("PROCESSED_DOCS_PATH", "processed_docs/processed_docs.pkl")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag = AINewsRAG(embedding_model)

print("ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")

# JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
documents = rag.load_json_files(news_dir)

# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
processed_docs = rag.process_documents(documents)
rag.create_vector_store(processed_docs)

# ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
rag.save_vector_store(vector_store_path, processed_doc_path)
print("âœ… ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")


# ### í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ í…ŒìŠ¤íŠ¸

# In[4]:


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
import os
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
vector_store_path = os.getenv("VECTOR_STORE_NAME", "ai_news_vectorstore")
news_dir = os.getenv("NEWS_FILE_PATH", "./ai_news")
processed_doc_path = os.getenv("PROCESSED_DOCS_PATH", "processed_docs/processed_docs.pkl")

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” 
embedding_model = OpenAIEmbeddings(
    model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
)

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag = AINewsRAG(embedding_model)

try:
    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
    rag.load_vector_store(vector_store_path, processed_doc_path)
    print("âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
except Exception as e:
    print(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

# ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œì‘
print("\nğŸ” AI ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
print("- ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
print("- ê²€ìƒ‰ ë°©ì‹ ë³€ê²½ì€ 'mode [semantic/keyword/hybrid]'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

search_mode = "hybrid"
while True:
    query = input("\nğŸ” ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not query:
        continue
        
    if query.lower() in ['q', 'quit']:
        print("\nğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
        
    if query.lower().startswith('mode '):
        mode = query.split()[1].lower()
        if mode in ['semantic', 'keyword', 'hybrid']:
            search_mode = mode
            print(f"\nâœ… ê²€ìƒ‰ ëª¨ë“œë¥¼ '{mode}'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâŒ ì˜ëª»ëœ ê²€ìƒ‰ ëª¨ë“œì…ë‹ˆë‹¤. semantic/keyword/hybrid ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
        continue

    try:
        print(f"\n'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ëª¨ë“œ: {search_mode})")
        
        if search_mode == "hybrid":
            results = rag.hybrid_search(query, k=5, semantic_weight=0.7)
        elif search_mode == "semantic":
            results = rag.vector_store.similarity_search_with_score(query, k=5)
        else:  # keyword
            results = rag.keyword_search(query, k=5)
        
        print(f"\nâœ¨ ê²€ìƒ‰ ì™„ë£Œ! {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
        
        # ê²°ê³¼ ì¶œë ¥
        for i, (doc, score) in enumerate(results, 1):
            print(f"\n{'='*80}")
            print(f"ê²€ìƒ‰ ê²°ê³¼ {i}/{len(results)}")
            print(f"ì œëª©: {doc.metadata['title']}")
            print(f"ë‚ ì§œ: {doc.metadata['date']}")
            if search_mode == "hybrid":
                print(f"í†µí•© ì ìˆ˜: {score:.4f}")
            elif search_mode == "semantic":
                print(f"ìœ ì‚¬ë„ ì ìˆ˜: {1 - (score/2):.4f}")
            else:
                print(f"BM25 ì ìˆ˜: {score:.4f}")
            print(f"URL: {doc.metadata['url']}")
            print(f"{'-'*40}")
            print(f"ë‚´ìš©:\n{doc.page_content[:300]}...")
            
    except Exception as e:
        print(f"\nâŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

