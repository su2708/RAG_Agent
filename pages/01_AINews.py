import os
from langchain_openai import OpenAIEmbeddings
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from langchain.schema.runnable import RunnablePassthrough
from AINewsRAG.NewsRAG import AINewsRAG, StreamHandler
from AI_agent.Youtube_AI import print_messages
from dotenv import load_dotenv
from tqdm import tqdm
import streamlit as st

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI News",
    page_icon="ğŸ“°",
)

st.title("ğŸ“°AI NewsğŸ“°")

st.markdown(
    """
    ### AI ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    
    - AIì™€ ê´€ë ¨ëœ ì£¼ì œì˜ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.
    - ë‹µë³€ì€ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.

    - AI ê´€ë ¨ ì£¼ì œ íŒë‹¨ ê¸°ì¤€:
        - AI ê¸°ìˆ  (ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬ ë“±)
        - AI ë„êµ¬ ë° ì„œë¹„ìŠ¤ (ChatGPT, DALL-E, Stable Diffusion ë“±)
        - AI íšŒì‚¬ ë° ì—°êµ¬ì†Œ ì†Œì‹
        - AI ì •ì±… ë° ê·œì œ
        - AI êµìœ¡ ë° í•™ìŠµ
        - AI ìœ¤ë¦¬ ë° ì˜í–¥
    """
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv() 

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

vector_store_path = "ai_news_vectorstore/"

# ì„ë² ë”© ëª¨ë¸
embed_model = OpenAIEmbeddings(model=os.getenv("OPENAI_EMBEDDING_MODEL"))

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag = AINewsRAG(embed_model)

try:
    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
    rag.load_vector_store(vector_store_path)
    print("âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
except Exception as e:
    print(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# Streamlit part ì‹œì‘

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

with st.sidebar:
    summarize = st.radio(
        "ë‰´ìŠ¤ë¥¼ LLMìœ¼ë¡œ ìš”ì•½í•´ì„œ ì„ë² ë”© í›„ ì €ì¥í• ê¹Œìš”?",
        (False, True)
    )

if summarize:
    # ê²½ë¡œ ì„¤ì • 
    json_files_path = "ai_news/"
    vector_store_path = "summarized_ai_news_vectorstore/"
    
    # ìš”ì•½ í´ë” í™•ì¸ 
    if not os.path.exists(vector_store_path):
        print(f"{vector_store_path} í´ë”ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        os.makedirs(vector_store_path)
    
        # JSON íŒŒì¼ë“¤ ë¡œë“œ
        documents = rag.load_json_files(json_files_path)
        
        # ë¬¸ì„œ ì²˜ë¦¬
        processed_docs = rag.process_documents(documents[:100])
        
        # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„± 
        summarize_prompt = ChatPromptTemplate.from_messages([
            ("system", "ì§ˆë¬¸ìœ¼ë¡œ ë“¤ì–´ì˜¨ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”."),
            ("human", "{doc}"),
        ])
        
        # ìš”ì•½ LLM ìƒì„±
        summarize_llm = ChatOpenAI(
            model="gpt-4o",
            api_key=OPENAI_API_KEY,
            temperature=0.1,
        )
        
        # ìš”ì•½ ì²´ì¸ ì„¤ì • 
        summarize_chain = {"doc": RunnablePassthrough()}|summarize_prompt|summarize_llm
        
        # ë¬¸ì„œ ìš”ì•½í•˜ê¸° 
        summarized_processed_docs = []
        for processed_doc in tqdm(processed_docs, desc="ë¬¸ì„œ ìš”ì•½ ì¤‘"):
            metadata = processed_doc.metadata
            doc = processed_doc.page_content
            
            # ë¬¸ì„œ ìš”ì•½ ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ 
            res = summarize_chain.invoke(doc).content
            
            summarized_processed_docs.append(Document(
                metadata=metadata,
                page_content=res
            ))
        
        # ìš”ì•½ëœ ë¬¸ì„œë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± 
        rag.create_vector_store(summarized_processed_docs)
        
        # ìš”ì•½ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ 
        rag.save_vector_store(vector_store_path)
        
        try:
            # ì €ì¥ëœ ìš”ì•½ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
            rag.load_vector_store(vector_store_path)
            print("âœ… ì‹ ê·œ ìš”ì•½ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ì‹ ê·œ ìš”ì•½ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            
    else:
        try:
            # ìš”ì•½ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
            rag.load_vector_store(vector_store_path)
            print("âœ… ê¸°ì¡´ ìš”ì•½ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ê¸°ì¡´ ìš”ì•½ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

# ëŒ€í™” ë‚´ì—­ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë°›ê¸° 
if user_input := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
    
    # ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = rag.search(user_input, k=5)
    context = "\n".join([doc.page_content for doc in relevant_docs])
    max_context_length = 3000  # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìµœëŒ€ ê¸¸ì´ ì œí•œ
    context = context[:max_context_length]
    
    # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    print_messages()

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    contextë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
                    ëª¨ë¥´ë©´ ë‹µë³€ì„ ì§€ì–´ë‚´ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
                    
                    ì•„ë˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì…ë‹ˆë‹¤:\n{context}
                    """
                ),
                ("human", "{question}"),
            ]
        )
        
        # ëª¨ë¸ ìƒì„±
        llm = ChatOpenAI(
            model="gpt-4o",
            api_key=OPENAI_API_KEY,
            temperature=0.1,
            streaming=True,
            callbacks=[stream_handler],
            max_tokens=500
        )

        chain = {
            "context": lambda x:context,
            "question": RunnablePassthrough(),
        } | prompt | llm

        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° AI ì‘ë‹µ ìƒì„±
        response = chain.invoke(
            {"question": user_input},
        )
        
        st.session_state["messages"].append(
            ChatMessage(role="assistant", content=response.content)
        )