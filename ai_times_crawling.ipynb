{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_list(page: int) -> str:\n",
    "    \"\"\"AI타임즈 뉴스 목록 페이지의 HTML을 가져오는 함수\"\"\"\n",
    "    url = \"https://www.aitimes.com/news/articleList.html\"\n",
    "    params = {\n",
    "        \"page\": page,\n",
    "        \"view_type\": \"sm\"\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"페이지 {page} 요청 중 에러 발생: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_article_content(url: str) -> str:\n",
    "    \"\"\"기사 상세 페이지의 내용을 가져오는 함수\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 기사 본문 찾기\n",
    "        article_content_div = soup.find(\"article\", id=\"article-view-content-div\")\n",
    "        if article_content_div:\n",
    "            # 모든 p 태그 찾기\n",
    "            paragraphs = article_content_div.find_all(\"p\")\n",
    "            # 각 단락의 텍스트를 리스트로 모으기\n",
    "            content_list = [p.text.strip() for p in paragraphs if p.text.strip()]\n",
    "            # 단락들을 개행문자로 결합\n",
    "            return '\\n'.join(content_list)\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"기사 내용 요청 중 에러 발생: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_news_info(html: str) -> List[Dict]:\n",
    "    \"\"\"HTML에서 뉴스 기사 정보를 파싱하는 함수\"\"\"\n",
    "    articles = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    content_list = soup.find(\"section\", id=\"section-list\")\n",
    "    if not content_list:\n",
    "        return articles\n",
    "        \n",
    "    news_items = content_list.find_all(\"li\")\n",
    "    print(f\"찾은 {len(news_items)}개의 기사\")\n",
    "    \n",
    "    for item in news_items:\n",
    "        # 제목 추출\n",
    "        title_elem = item.find(\"h4\", class_=\"titles\")\n",
    "        title = title_elem.find(\"a\").text.strip() if title_elem else \"\"\n",
    "        \n",
    "        # URL 추출\n",
    "        url_elem = title_elem.find(\"a\") if title_elem else None\n",
    "        url = \"https://www.aitimes.com\" + url_elem[\"href\"] if url_elem and \"href\" in url_elem.attrs else \"\"\n",
    "        \n",
    "        # 설명 추출\n",
    "        desc_elem = item.find(\"p\", class_=\"lead\")\n",
    "        description = desc_elem.find(\"a\").text.strip() if desc_elem else \"\"\n",
    "        \n",
    "        # 날짜 추출\n",
    "        date_elem = item.find(\"span\", class_=\"byline\")\n",
    "        if date_elem:\n",
    "            date = date_elem.find_all(\"em\")[-1].text.strip()\n",
    "        else:\n",
    "            date = \"\"\n",
    "            \n",
    "        # 기사 본문 가져오기\n",
    "        content = get_article_content(url) if url else \"\"\n",
    "        \n",
    "        article = {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"url\": url,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "        articles.append(article)\n",
    "        # 서버 부하를 줄이기 위한 지연\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def get_date_range(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"데이터프레임에서 가장 빠른 날짜와 가장 늦은 날짜를 추출하는 함수\"\"\"\n",
    "    try:\n",
    "        # 날짜 문자열을 datetime 객체로 변환\n",
    "        df['parsed_date'] = pd.to_datetime(df['date'], format='%Y.%m.%d %H:%M')\n",
    "        \n",
    "        # 가장 빠른 날짜와 가장 늦은 날짜 추출\n",
    "        start_date = df['parsed_date'].min().strftime('%Y%m%d')\n",
    "        end_date = df['parsed_date'].max().strftime('%Y%m%d')\n",
    "        \n",
    "        # 임시 컬럼 삭제\n",
    "        df.drop('parsed_date', axis=1, inplace=True)\n",
    "        \n",
    "        return start_date, end_date\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 처리 중 에러 발생: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def save_to_files(articles: List[Dict], start_page: int, end_page: int, base_path: str = \"./files\"):\n",
    "    \"\"\"뉴스 기사 정보를 DataFrame으로 변환하여 CSV와 JSON 파일로 저장하는 함수\"\"\"\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # 날짜 범위 추출\n",
    "    start_date, end_date = get_date_range(df)\n",
    "    \n",
    "    if not (start_date and end_date):\n",
    "        # 날짜 추출 실패시 현재 시간 사용\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        file_name = f\"ai_times_news_p{start_page}-{end_page}_{current_time}.json\"\n",
    "    else:\n",
    "        # 날짜 범위와 페이지 정보를 포함한 파일명 생성\n",
    "        file_name = f\"ai_times_news_{start_date}-{end_date}_p{start_page}-{end_page}.json\"\n",
    "    \n",
    "    \n",
    "    # DataFrame을 JSON 파일로 저장\n",
    "    json_path = os.path.join(base_path, file_name)\n",
    "    df.to_json(json_path, force_ascii=False, orient='records', indent=4)\n",
    "    print(f\"JSON 파일 저장 완료: {json_path}\")\n",
    "    \n",
    "    # 데이터 수집 정보 출력\n",
    "    print(\"\\n[데이터 수집 정보]\")\n",
    "    print(f\"수집 페이지: {start_page}-{end_page} 페이지\")\n",
    "    print(f\"수집 기간: {start_date[:4]}.{start_date[4:6]}.{start_date[6:]} - {end_date[:4]}.{end_date[4:6]}.{end_date[6:]}\")\n",
    "    print(f\"수집 기사: 총 {len(df)}건\")\n",
    "    \n",
    "    # 데이터 미리보기\n",
    "    print(\"\\n[데이터 미리보기]\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_page = 1\n",
    "    end_page = 1  # 수집할 마지막 페이지 번호\n",
    "    page_unit = 10  # 한 번에 처리할 페이지 수\n",
    "    download_folder = './files/ai_news' #저장 폴더 이름\n",
    "    os.makedirs(download_folder, exist_ok=True) #폴더 생성\n",
    "    print(\"뉴스 기사 정보 수집 중...\")\n",
    "    \n",
    "    # 10페이지씩 처리\n",
    "    for start_idx in range(start_page, end_page + 1, page_unit):\n",
    "        all_articles = []\n",
    "        end_idx = min(start_idx + page_unit - 1, end_page)\n",
    "        \n",
    "        print(f\"\\n=== {start_idx}~{end_idx} 페이지 수집 시작 ===\")\n",
    "        \n",
    "        for page in range(start_idx, end_idx + 1):\n",
    "            html = get_news_list(page)\n",
    "            if html:\n",
    "                page_articles = parse_news_info(html)\n",
    "                all_articles.extend(page_articles)\n",
    "                print(f\"페이지 {page}: {len(page_articles)}개의 기사 정보 수집 완료\")\n",
    "        \n",
    "        if not all_articles:\n",
    "            print(f\"{start_idx}~{end_idx} 페이지의 기사 정보를 가져오는데 실패했습니다.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{start_idx}~{end_idx} 페이지 : 총 {len(all_articles)}개의 기사 정보를 수집했습니다.\")\n",
    "        \n",
    "        try:\n",
    "            save_to_files(all_articles, start_idx, end_idx, download_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"데이터 저장 중 에러 발생: {e}\")\n",
    "        \n",
    "        print(f\"=== {start_idx}~{end_idx} 페이지 처리 완료 ===\\n\")\n",
    "        \n",
    "        # 다음 수집 전 잠시 대기\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(\"\\n모든 페이지 수집 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사 정보 수집 중...\n",
      "\n",
      "=== 1~1 페이지 수집 시작 ===\n",
      "찾은 20개의 기사\n",
      "페이지 1: 20개의 기사 정보 수집 완료\n",
      "\n",
      "1~1 페이지 : 총 20개의 기사 정보를 수집했습니다.\n",
      "JSON 파일 저장 완료: ./ai_news\\ai_times_news_20241121-20241122_p1-1.json\n",
      "\n",
      "[데이터 수집 정보]\n",
      "수집 페이지: 1-1 페이지\n",
      "수집 기간: 2024.11.21 - 2024.11.22\n",
      "수집 기사: 총 20건\n",
      "\n",
      "[데이터 미리보기]\n",
      "                                     title  \\\n",
      "0  법제연구원-구글코리아-개인정보위, 학술대회서 ‘AI 입법 방향성’ 논의   \n",
      "1             독일, 고속도로에 54GW 태양광 설치 가능성 평가   \n",
      "2     구글클라우드, 워크스페이스 사이드 패널서 '제미나이' 한국어 지원   \n",
      "3   플루, 올리브영 '올영라이브'서 25일 라이브 방송서 특별 구성 공개   \n",
      "4                    켄텍, 영농형태양광협회와 업무협약 체결   \n",
      "\n",
      "                                         description  \\\n",
      "0  한국법제연구원(원장 한영수)은 서울 강남 파이낸셜센터 21층 대회의실에서 ‘인공지능...   \n",
      "1  독일 연방도로청은 20일(현지시간) 고속도로 유휴부지 및 관련 시설에 태양광 발전 ...   \n",
      "2  구글 클라우드가 워크스페이스를 위한 제미나이(Gemini for Google Wor...   \n",
      "3  지본코스메틱의 대표 브랜드 플루(PLU)는 25일 낮 12시부터 12시30분까지 올...   \n",
      "4  한국에너지공과대학교(켄텍)은 21일 한국영농형태양광협회와 영농형 태양광의 기술개발 ...   \n",
      "\n",
      "                                                 url              date  \\\n",
      "0  https://www.aitimes.com/news/articleView.html?...  2024.11.22 16:14   \n",
      "1  https://www.aitimes.com/news/articleView.html?...  2024.11.22 15:07   \n",
      "2  https://www.aitimes.com/news/articleView.html?...  2024.11.22 14:00   \n",
      "3  https://www.aitimes.com/news/articleView.html?...  2024.11.22 13:56   \n",
      "4  https://www.aitimes.com/news/articleView.html?...  2024.11.22 13:49   \n",
      "\n",
      "                                             content  \n",
      "0  한국법제연구원(원장 한영수)은 서울 강남 파이낸셜센터 21층 대회의실에서 ‘인공지능...  \n",
      "1  독일 연방도로청은 20일(현지시간) 고속도로 유휴부지 및 관련 시설에 태양광 발전 ...  \n",
      "2  구글 클라우드가 워크스페이스를 위한 제미나이(Gemini for Google Wor...  \n",
      "3  지본코스메틱의 대표 브랜드 플루(PLU)는 25일 낮 12시부터 12시30분까지 올...  \n",
      "4  한국에너지공과대학교(켄텍)은 21일 한국영농형태양광협회와 영농형 태양광의 기술개발 ...  \n",
      "=== 1~1 페이지 처리 완료 ===\n",
      "\n",
      "\n",
      "모든 페이지 수집 완료!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
