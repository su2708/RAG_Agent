{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이브리드 검색 시스템\n",
    "\n",
    "이 코드는 AI 관련 뉴스 기사를 효과적으로 검색하기 위한 하이브리드 검색 시스템을 구현한 것입니다.\n",
    "벡터 기반 의미론적 검색과 키워드 기반 검색을 결합하여 더 정확한 검색 결과를 제공합니다.\n",
    "\n",
    "## 주요 기능\n",
    "\n",
    "1. **문서 처리**\n",
    "   - JSON 형식의 뉴스 데이터 로드\n",
    "   - 문서를 청크 단위로 분할\n",
    "   - 벡터 DB 및 키워드 검색용 인덱스 생성\n",
    "\n",
    "2. **하이브리드 검색**\n",
    "   - 벡터 기반 의미론적 검색 (FAISS)\n",
    "   - 키워드 기반 검색 (BM25)\n",
    "   - 두 검색 방식의 결과를 가중치를 적용하여 통합\n",
    "\n",
    "3. **데이터 관리**\n",
    "   - 벡터 스토어 저장/로드\n",
    "   - 처리된 문서 데이터 저장/로드\n",
    "   - 진행 상황 로깅\n",
    "\n",
    "\n",
    "## 검색 가중치 설정 가이드\n",
    "\n",
    "- **의미론적 검색 중심 (semantic_weight=0.7)**\n",
    "  - 문맥과 의미를 더 중요하게 고려\n",
    "  - 유사한 주제의 문서도 검색 가능\n",
    "  - 예: \"AI 기술의 미래 전망\" → AI 발전 방향, 기술 트렌드 등 관련 문서 포함\n",
    "\n",
    "- **키워드 검색 중심 (semantic_weight=0.3)**\n",
    "  - 정확한 키워드 매칭을 중시\n",
    "  - 특정 용어나 개념이 포함된 문서 우선\n",
    "  - 예: \"삼성전자 AI 칩\" → 정확히 해당 키워드가 포함된 문서 우선\n",
    "\n",
    "- **균형잡힌 검색 (semantic_weight=0.5)**\n",
    "  - 두 방식의 장점을 균형있게 활용\n",
    "  - 일반적인 검색에 적합\n",
    "  - 예: \"자율주행 안전\" → 키워드 매칭과 의미적 연관성 모두 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class AINewsRAG:\n",
    "    \"\"\"\n",
    "    AI 뉴스 검색을 위한 RAG(Retrieval-Augmented Generation) 시스템\n",
    "    \n",
    "    이 클래스는 뉴스 기사를 벡터 DB로 변환하고, 의미론적 검색과 키워드 기반 검색을\n",
    "    결합한 하이브리드 검색 기능을 제공합니다.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings: OpenAI 임베딩 모델\n",
    "        text_splitter: 문서 분할을 위한 스플리터\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        bm25: 키워드 기반 검색을 위한 BM25 모델\n",
    "        processed_docs: 처리된 문서들의 리스트\n",
    "        doc_mapping: 문서 ID와 문서 객체 간의 매핑\n",
    "        logger: 로깅을 위한 로거 객체\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model):\n",
    "        \"\"\"\n",
    "        AINewsRAG 클래스 초기화\n",
    "\n",
    "        Args:\n",
    "            embedding_model: OpenAI 임베딩 모델 인스턴스\n",
    "        \"\"\"\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self.bm25 = None\n",
    "        self.processed_docs = None\n",
    "        self.doc_mapping = None\n",
    "        \n",
    "        # 로깅 설정\n",
    "        self.logger = logging.getLogger('AINewsRAG')\n",
    "        # 기존 핸들러 제거\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "        # 로그 중복 방지\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "    def load_json_files(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        여러 JSON 파일에서 뉴스 기사를 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): JSON 파일들이 있는 디렉토리 경로\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 로드된 뉴스 기사 리스트\n",
    "\n",
    "        Raises:\n",
    "            Exception: 파일 로드 중 오류 발생 시\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        json_files = glob.glob(f\"{directory_path}/ai_times_news_*.json\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(json_files)}개의 JSON 파일을 로드합니다...\")\n",
    "        \n",
    "        for file_path in tqdm(json_files, desc=\"JSON 파일 로드 중\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    documents = json.load(file)\n",
    "                    if documents:\n",
    "                        documents = [doc for doc in documents if len(doc['content']) > 10]\n",
    "                    if len(documents) >= 10:\n",
    "                        all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"파일 로드 중 오류 발생: {file_path} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"총 {len(all_documents)}개의 뉴스 기사를 로드했습니다.\")\n",
    "        return all_documents\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict]) -> List[Document]:\n",
    "        \"\"\"문서를 처리하고 청크로 분할합니다.\"\"\"\n",
    "        processed_docs = []\n",
    "        self.logger.info(\"문서 처리 및 청크 분할을 시작합니다...\")\n",
    "        \n",
    "        for idx, doc in enumerate(tqdm(documents, desc=\"문서 처리 중\")):\n",
    "            try:\n",
    "                #첫 정크에 문서의 제목 포함\n",
    "                full_text = f\"{doc['title']}\\n {doc['content']}\"\n",
    "                metadata = {\n",
    "                    'doc_id': idx, \n",
    "                    'title': doc['title'],\n",
    "                    'url': doc['url'],\n",
    "                    'date': doc['date']\n",
    "                }\n",
    "                \n",
    "                chunks = self.text_splitter.split_text(full_text)\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'chunk_id': f\"doc_{idx}_chunk_{chunk_idx}\"  # 청크별 고유 ID\n",
    "                        }\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"문서 처리 중 오류 발생: {doc.get('title', 'Unknown')} - {str(e)}\")\n",
    "        \n",
    "        self.processed_docs = processed_docs\n",
    "        self.initialize_bm25(processed_docs)\n",
    "        \n",
    "        return processed_docs\n",
    "\n",
    "    def initialize_bm25(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        BM25 검색 엔진을 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): 처리된 문서 리스트\n",
    "        \"\"\"\n",
    "        self.logger.info(\"BM25 검색 엔진을 초기화합니다...\")\n",
    "        \n",
    "        tokenized_corpus = [\n",
    "            doc.page_content.lower().split() \n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.doc_mapping = {\n",
    "            i: doc for i, doc in enumerate(documents)\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"BM25 검색 엔진 초기화가 완료되었습니다.\")\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        FAISS 벡터 스토어를 생성합니다.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): 벡터화할 문서 리스트\n",
    "\n",
    "        Raises:\n",
    "            Exception: 벡터 스토어 생성 중 오류 발생 시\n",
    "        \"\"\"\n",
    "        self.logger.info(\"벡터 스토어 생성을 시작합니다...\")\n",
    "        total_docs = len(documents)\n",
    "        \n",
    "        try:\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"벡터 생성 중\"):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                if self.vector_store is None:\n",
    "                    self.vector_store = FAISS.from_documents(batch, self.embeddings)\n",
    "                else:\n",
    "                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)\n",
    "                    self.vector_store.merge_from(batch_vectorstore)\n",
    "            \n",
    "            self.logger.info(\"벡터 스토어 생성이 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"벡터 스토어 생성 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        키워드 기반 BM25 검색을 수행합니다.\n",
    "\n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            k (int): 반환할 결과 수\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: (문서, 점수) 튜플의 리스트\n",
    "\n",
    "        Raises:\n",
    "            ValueError: BM25가 초기화되지 않은 경우\n",
    "        \"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25가 초기화되지 않았습니다.\")\n",
    "        \n",
    "        self.logger.info(f\"'{query}' 키워드 검색을 시작합니다...\")\n",
    "        \n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        top_k_idx = np.argsort(bm25_scores)[-k:][::-1]\n",
    "        results = [\n",
    "            (self.doc_mapping[idx], bm25_scores[idx])\n",
    "            for idx in top_k_idx\n",
    "        ]\n",
    "        \n",
    "        self.logger.info(f\"{len(results)}개의 키워드 검색 결과를 찾았습니다.\")\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(\n",
    "            self, \n",
    "            query: str, \n",
    "            k: int = 5, \n",
    "            semantic_weight: float = 0.5\n",
    "        ) -> List[Tuple[Document, float]]:\n",
    "            \"\"\"\n",
    "            의미론적 검색과 키워드 검색을 결합한 하이브리드 검색을 수행합니다.\n",
    "            \"\"\"\n",
    "            self.logger.info(f\"'{query}' 하이브리드 검색을 시작합니다...\")\n",
    "            \n",
    "            # 의미론적 검색 수행\n",
    "            self.logger.info(f\"'{query}' 의미론적 검색을 시작합니다...\")\n",
    "            semantic_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "            self.logger.info(f\"{len(semantic_results)}개의 의미론적 검색 결과를 찾았습니다.\")\n",
    "\n",
    "            # 키워드 기반 검색 수행\n",
    "            keyword_results = self.keyword_search(query, k=k)\n",
    "            \n",
    "            # 문서 ID를 키로 사용\n",
    "            combined_scores = {}\n",
    "            \n",
    "            # 의미론적 검색 결과 처리\n",
    "            max_semantic_score = max(score for _, score in semantic_results)\n",
    "            for doc, score in semantic_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                \n",
    "                #5개의 문서의 점수가\n",
    "                normalized_score = 1 - (score / max_semantic_score) \n",
    "                combined_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'score': semantic_weight * normalized_score\n",
    "                }\n",
    "            \n",
    "            # 키워드 검색 결과 처리\n",
    "            max_keyword_score = max(score for _, score in keyword_results)\n",
    "            for doc, score in keyword_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                normalized_score = score / max_keyword_score\n",
    "                if doc_id in combined_scores:\n",
    "                    combined_scores[doc_id]['score'] += (1 - semantic_weight) * normalized_score\n",
    "                else:\n",
    "                    combined_scores[doc_id] = {\n",
    "                        'doc': doc,\n",
    "                        'score': (1 - semantic_weight) * normalized_score\n",
    "                    }\n",
    "            \n",
    "            # 결과 정렬\n",
    "            sorted_results = sorted(\n",
    "                [(info['doc'], info['score']) for info in combined_scores.values()],\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "            \n",
    "            self.logger.info(f\"{len(sorted_results)}개의 하이브리드 검색 결과를 찾았습니다.\")\n",
    "            return sorted_results\n",
    "\n",
    "    def save_vector_store(self, vector_store_path: str, processed_docs_path:str=None):\n",
    "        \"\"\"\n",
    "        벡터 스토어와 BM25 데이터를 저장합니다.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"데이터를 {vector_store_path}에 저장합니다...\")\n",
    "            \n",
    "            # 벡터 스토어 저장\n",
    "            os.makedirs(vector_store_path, exist_ok=True)\n",
    "            self.vector_store.save_local(vector_store_path)\n",
    "            \n",
    "            # processed_docs 저장\n",
    "            if self.processed_docs:\n",
    "                os.makedirs(os.path.dirname(processed_docs_path), exist_ok=True)\n",
    "                with open(processed_docs_path, 'wb') as f:\n",
    "                    pickle.dump(self.processed_docs, f)\n",
    "            \n",
    "            self.logger.info(\"저장이 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"저장 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_vector_store(self, vector_store_path: str, processed_docs_path):\n",
    "        \"\"\"\n",
    "        벡터 스토어와 BM25 데이터를 로드합니다.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"데이터를 {vector_store_path}에서 로드합니다...\")\n",
    "            \n",
    "            # 벡터 스토어 로드\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                vector_store_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # processed_docs 로드\n",
    "            if os.path.exists(processed_docs_path):\n",
    "                with open(processed_docs_path, 'rb') as f:\n",
    "                    self.processed_docs = pickle.load(f)\n",
    "                self.initialize_bm25(self.processed_docs)\n",
    "            \n",
    "            self.logger.info(\"로드가 완료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"로드 중 오류 발생: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI뉴스 데이터 Vector DB 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# 임베딩 모델 초기화 \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# 환경 변수에서 경로 가져오기\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "print(\"새로운 벡터 스토어를 생성합니다...\")\n",
    "\n",
    "# JSON 파일에서 뉴스 데이터 로드\n",
    "documents = rag.load_json_files(news_dir)\n",
    "\n",
    "# 문서 처리 및 벡터 스토어 생성\n",
    "processed_docs = rag.process_documents(documents)\n",
    "rag.create_vector_store(processed_docs)\n",
    "\n",
    "# 벡터 스토어 저장\n",
    "rag.save_vector_store(vector_store_path, processed_doc_path)\n",
    "print(\"✅ 새로운 벡터 스토어 생성 및 저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이브리드 서치 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:38:56,843 - 데이터를 ai_news_vectorstore에서 로드합니다...\n",
      "2024-11-28 11:38:58,449 - BM25 검색 엔진을 초기화합니다...\n",
      "2024-11-28 11:39:04,605 - BM25 검색 엔진 초기화가 완료되었습니다.\n",
      "2024-11-28 11:39:04,676 - 로드가 완료되었습니다.\n",
      "✅ 기존 벡터 스토어를 로드했습니다.\n",
      "\n",
      "🔍 AI 뉴스 검색 시스템을 시작합니다.\n",
      "- 종료하려면 'q' 또는 'quit'를 입력하세요.\n",
      "\n",
      "'AI 뉴스' 검색을 시작합니다... (모드: hybrid)\n",
      "2024-11-28 11:39:08,417 - 'AI 뉴스' 하이브리드 검색을 시작합니다...\n",
      "2024-11-28 11:39:08,808 - 'AI 뉴스' 키워드 검색을 시작합니다...\n",
      "2024-11-28 11:39:08,859 - 5개의 키워드 검색 결과를 찾았습니다.\n",
      "2024-11-28 11:39:08,863 - 5개의 하이브리드 검색 결과를 찾았습니다.\n",
      "\n",
      "✨ 검색 완료! 5개의 결과를 찾았습니다.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "검색 결과 1/5\n",
      "제목: 네이버, 뉴스 AI 알고리즘 안내 페이지 강화\n",
      "날짜: 2023.07.07 12:54\n",
      "통합 점수: 0.3000\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=152292\n",
      "----------------------------------------\n",
      "내용:\n",
      "네이버, 뉴스 AI 알고리즘 안내 페이지 강화\n",
      " 네이버(대표 최수연)는 뉴스 인공지능(AI) 알고리즘 안내 페이지를 구체적이고 이용자가 쉽게 이해할 수 있도록 새롭게 선보였다고 7일 밝혔다.\n",
      "네이버 뉴스 AI 알고리즘 안내 페이지는 ▲도입 배경 ▲AiRS(AI 추천 시스템) ▲알고리즘 팩터 ▲클러스터링 ▲기술 고도화 ▲FAQ로 구성했다. 기존 서비스 이용안내-서비스 운영원칙-뉴스 AI 알고리즘과 검색 블로그 등에 게시했던 관련 내용보다 투명하고 일목요연하게 분류했다고 설명했다. 특히 개인화 및 비개인화를 기준으로 각 피처들의 의미를...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 2/5\n",
      "제목: 포티투마루, 로이터와 뉴스 콘텐츠 제휴\n",
      "날짜: 2024.10.18 14:05\n",
      "통합 점수: 0.2907\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=164365\n",
      "----------------------------------------\n",
      "내용:\n",
      "포티투마루, 로이터와 뉴스 콘텐츠 제휴\n",
      " 생성 인공지능(AI) 전문 포티투마루(대표 김동환)는 글로벌 뉴스 제공 전문 로이터와 생성 AI 시대 뉴 미디어 서비스 구축을 위한 전략적 파트너십을 체결했다고 18일 밝혔다.\n",
      "이번 파트너십으로 로이터 영상을 포함한 글로벌 뉴스 콘텐츠를 포티투마루 IT 전문 매체에서 활용할 수 있게 됐다. 포티투마루는 뉴스 콘텐츠 기반의 뉴 미디어 서비스를 구축하는 등 전반적인 비즈니스 파트너십을 진행하겠다고 밝혔다.\n",
      "김동환 포티투마루 대표는 “이번 협력을 계기로 멀티모달 기반 생성 AI 시대에 미디어 산업...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 3/5\n",
      "제목: 구글도 뉴스 저작권 문제로 골머리...\"연간 수백만달러는 적어\"\n",
      "날짜: 2024.05.01 17:10\n",
      "통합 점수: 0.2830\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=159264\n",
      "----------------------------------------\n",
      "내용:\n",
      "구글도 뉴스 저작권 문제로 골머리...\"연간 수백만달러는 적어\"\n",
      " 오픈AI가 저작권 문제로 줄소송을 당하는 사이에, 구글은 월스트리트저널(WSJ)과 연간 500만~600만달러(약 69억~83억원)에 달하는 계약을 맺었다. 하지만 이는 인공지능(AI) 학습에 기사를 사용해도 된다는 라이선스 계약이 아니며, 저작권 문제에 대해서는 아직 액수 차가 큰 것으로 알려졌다.\n",
      "디 인포메이션은 30일(현지시간) 관계자를 인용, WSJ을 소유한 뉴스 코퍼레이션(News Corp)이 구글과 새로운 파트너십을 맺었다고 보도했다.\n",
      "이에 따르면 이번 계약...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 4/5\n",
      "제목: NC, 자연어 인식하는 로봇 솔루션 개발 나섰다\n",
      "날짜: 2023.09.12 16:42\n",
      "통합 점수: 0.2826\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=153584\n",
      "----------------------------------------\n",
      "내용:\n",
      "NC 역시 구글과 같은 방식으로 언어 모델을 학습하는 방식으로 로봇 제어 솔루션 개발에 나선 것이다.\n",
      "한편 NC는 2011년부터 AI 조직을 운영 중이다. 현재는 약 300의 전문 인력을 갖춰 AI와 NLP등 고도의 기술을 연구개발하고 있다.\n",
      "이미 항공기상청과 업무협약을 맺고 AI 기술을 활용해 항공 기상정보를 제작하기로 했다. 또 차량용 뉴스 서비스를 위한  '모빌리티 AI 뉴스 인프라 구축'에도 나섰다.\n",
      "장세민 기자 semim99@aitimes.com...\n",
      "\n",
      "================================================================================\n",
      "검색 결과 5/5\n",
      "제목: 네이버, 뉴스 AI 알고리즘 안내 페이지 강화\n",
      "날짜: 2023.07.07 12:54\n",
      "통합 점수: 0.2795\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=152292\n",
      "----------------------------------------\n",
      "내용:\n",
      "한편 2019년부터 자체 기사 배열을 중단한 네이버는 언론사가 직접 편집한 기사를 사용자가 선택하는 구독 기반 서비스 ‘언론사 편집’과 함께 보완적으로 AI 알고리즘에 기반한 추천 서비스를 제공 중이다. AiRS 개인화 추천, 뉴스 클러스터링 등의 AI 알고리즘은 모바일 MY뉴스, 모바일 및 PC 섹션별 뉴스 영역 등에서 작동 중이다.\n",
      "이주영 기자 juyoung09@aitimes.com...\n",
      "\n",
      "👋 검색을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 경로 가져오기\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# 임베딩 모델 초기화 \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "try:\n",
    "    # 기존 벡터 스토어 로드 시도\n",
    "    rag.load_vector_store(vector_store_path, processed_doc_path)\n",
    "    print(\"✅ 기존 벡터 스토어를 로드했습니다.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"벡터 스토어 로드 실패: {str(e)}\")\n",
    "\n",
    "# 대화형 검색 시작\n",
    "print(\"\\n🔍 AI 뉴스 검색 시스템을 시작합니다.\")\n",
    "print(\"- 종료하려면 'q' 또는 'quit'를 입력하세요.\")\n",
    "\n",
    "search_mode = \"hybrid\" # 검색 방식 변경은 'mode [semantic/keyword/hybrid]'를 입력하세요.\n",
    "while True:\n",
    "    query = input(\"\\n🔍 검색할 내용을 입력하세요: \").strip()\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "        \n",
    "    if query.lower() in ['q', 'quit']:\n",
    "        print(\"\\n👋 검색을 종료합니다.\")\n",
    "        break\n",
    "        \n",
    "    if query.lower().startswith('mode '):\n",
    "        mode = query.split()[1].lower()\n",
    "        if mode in ['semantic', 'keyword', 'hybrid']:\n",
    "            search_mode = mode\n",
    "            print(f\"\\n✅ 검색 모드를 '{mode}'로 변경했습니다.\")\n",
    "        else:\n",
    "            print(\"\\n❌ 잘못된 검색 모드입니다. semantic/keyword/hybrid 중 선택하세요.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n'{query}' 검색을 시작합니다... (모드: {search_mode})\")\n",
    "        \n",
    "        if search_mode == \"hybrid\":\n",
    "            results = rag.hybrid_search(query, k=5, semantic_weight=0.5)\n",
    "        elif search_mode == \"semantic\":\n",
    "            results = rag.vector_store.similarity_search_with_score(query, k=5)\n",
    "        else:  # keyword\n",
    "            results = rag.keyword_search(query, k=5)\n",
    "        \n",
    "        print(f\"\\n✨ 검색 완료! {len(results)}개의 결과를 찾았습니다.\\n\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"검색 결과 {i}/{len(results)}\")\n",
    "            print(f\"제목: {doc.metadata['title']}\")\n",
    "            print(f\"날짜: {doc.metadata['date']}\")\n",
    "            if search_mode == \"hybrid\":\n",
    "                print(f\"통합 점수: {score:.4f}\")\n",
    "            elif search_mode == \"semantic\":\n",
    "                print(f\"유사도 점수: {1 - (score/2):.4f}\")\n",
    "            else:\n",
    "                print(f\"BM25 점수: {score:.4f}\")\n",
    "            print(f\"URL: {doc.metadata['url']}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            print(f\"내용:\\n{doc.page_content[:300]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 검색 중 오류가 발생했습니다: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
