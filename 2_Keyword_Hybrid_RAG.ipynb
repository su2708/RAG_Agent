{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
    "\n",
    "ì´ ì½”ë“œëŠ” AI ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "1. **ë¬¸ì„œ ì²˜ë¦¬**\n",
    "   - JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ\n",
    "   - ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "   - ë²¡í„° DB ë° í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "2. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**\n",
    "   - ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ (FAISS)\n",
    "   - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25)\n",
    "   - ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í†µí•©\n",
    "\n",
    "3. **ë°ì´í„° ê´€ë¦¬**\n",
    "   - ë²¡í„° ìŠ¤í† ì–´ ì €ì¥/ë¡œë“œ\n",
    "   - ì²˜ë¦¬ëœ ë¬¸ì„œ ë°ì´í„° ì €ì¥/ë¡œë“œ\n",
    "   - ì§„í–‰ ìƒí™© ë¡œê¹…\n",
    "\n",
    "\n",
    "## ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì • ê°€ì´ë“œ\n",
    "\n",
    "- **ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì¤‘ì‹¬ (semantic_weight=0.7)**\n",
    "  - ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ê³ ë ¤\n",
    "  - ìœ ì‚¬í•œ ì£¼ì œì˜ ë¬¸ì„œë„ ê²€ìƒ‰ ê°€ëŠ¥\n",
    "  - ì˜ˆ: \"AI ê¸°ìˆ ì˜ ë¯¸ë˜ ì „ë§\" â†’ AI ë°œì „ ë°©í–¥, ê¸°ìˆ  íŠ¸ë Œë“œ ë“± ê´€ë ¨ ë¬¸ì„œ í¬í•¨\n",
    "\n",
    "- **í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ì‹¬ (semantic_weight=0.3)**\n",
    "  - ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì¤‘ì‹œ\n",
    "  - íŠ¹ì • ìš©ì–´ë‚˜ ê°œë…ì´ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ \n",
    "  - ì˜ˆ: \"ì‚¼ì„±ì „ì AI ì¹©\" â†’ ì •í™•íˆ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ \n",
    "\n",
    "- **ê· í˜•ì¡íŒ ê²€ìƒ‰ (semantic_weight=0.5)**\n",
    "  - ë‘ ë°©ì‹ì˜ ì¥ì ì„ ê· í˜•ìˆê²Œ í™œìš©\n",
    "  - ì¼ë°˜ì ì¸ ê²€ìƒ‰ì— ì í•©\n",
    "  - ì˜ˆ: \"ììœ¨ì£¼í–‰ ì•ˆì „\" â†’ í‚¤ì›Œë“œ ë§¤ì¹­ê³¼ ì˜ë¯¸ì  ì—°ê´€ì„± ëª¨ë‘ ê³ ë ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class AINewsRAG:\n",
    "    \"\"\"\n",
    "    AI ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ\n",
    "    \n",
    "    ì´ í´ë˜ìŠ¤ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë²¡í„° DBë¡œ ë³€í™˜í•˜ê³ , ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„\n",
    "    ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings: OpenAI ì„ë² ë”© ëª¨ë¸\n",
    "        text_splitter: ë¬¸ì„œ ë¶„í• ì„ ìœ„í•œ ìŠ¤í”Œë¦¬í„°\n",
    "        vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ\n",
    "        bm25: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ BM25 ëª¨ë¸\n",
    "        processed_docs: ì²˜ë¦¬ëœ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        doc_mapping: ë¬¸ì„œ IDì™€ ë¬¸ì„œ ê°ì²´ ê°„ì˜ ë§¤í•‘\n",
    "        logger: ë¡œê¹…ì„ ìœ„í•œ ë¡œê±° ê°ì²´\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model):\n",
    "        \"\"\"\n",
    "        AINewsRAG í´ë˜ìŠ¤ ì´ˆê¸°í™”\n",
    "\n",
    "        Args:\n",
    "            embedding_model: OpenAI ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤\n",
    "        \"\"\"\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self.bm25 = None\n",
    "        self.processed_docs = None\n",
    "        self.doc_mapping = None\n",
    "        \n",
    "        # ë¡œê¹… ì„¤ì •\n",
    "        self.logger = logging.getLogger('AINewsRAG')\n",
    "        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "        # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "    def load_json_files(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ì—¬ëŸ¬ JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: ë¡œë“œëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        Raises:\n",
    "            Exception: íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        json_files = glob.glob(f\"{directory_path}/ai_times_news_*.json\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        for file_path in tqdm(json_files, desc=\"JSON íŒŒì¼ ë¡œë“œ ì¤‘\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    documents = json.load(file)\n",
    "                    if documents:\n",
    "                        documents = [doc for doc in documents if len(doc['content']) > 10]\n",
    "                    if len(documents) >= 10:\n",
    "                        all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {str(e)}\")\n",
    "        \n",
    "        self.logger.info(f\"ì´ {len(all_documents)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return all_documents\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict]) -> List[Document]:\n",
    "        \"\"\"ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\"\"\"\n",
    "        processed_docs = []\n",
    "        self.logger.info(\"ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        for idx, doc in enumerate(tqdm(documents, desc=\"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘\")):\n",
    "            try:\n",
    "                #ì²« ì •í¬ì— ë¬¸ì„œì˜ ì œëª© í¬í•¨\n",
    "                full_text = f\"{doc['title']}\\n {doc['content']}\"\n",
    "                metadata = {\n",
    "                    'doc_id': idx, \n",
    "                    'title': doc['title'],\n",
    "                    'url': doc['url'],\n",
    "                    'date': doc['date']\n",
    "                }\n",
    "                \n",
    "                chunks = self.text_splitter.split_text(full_text)\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    processed_docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'chunk_id': f\"doc_{idx}_chunk_{chunk_idx}\"  # ì²­í¬ë³„ ê³ ìœ  ID\n",
    "                        }\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {doc.get('title', 'Unknown')} - {str(e)}\")\n",
    "        \n",
    "        self.processed_docs = processed_docs\n",
    "        self.initialize_bm25(processed_docs)\n",
    "        \n",
    "        return processed_docs\n",
    "\n",
    "    def initialize_bm25(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        self.logger.info(\"BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        tokenized_corpus = [\n",
    "            doc.page_content.lower().split() \n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.doc_mapping = {\n",
    "            i: doc for i, doc in enumerate(documents)\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"BM25 ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): ë²¡í„°í™”í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        Raises:\n",
    "            Exception: ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ\n",
    "        \"\"\"\n",
    "        self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        total_docs = len(documents)\n",
    "        \n",
    "        try:\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"ë²¡í„° ìƒì„± ì¤‘\"):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                if self.vector_store is None:\n",
    "                    self.vector_store = FAISS.from_documents(batch, self.embeddings)\n",
    "                else:\n",
    "                    batch_vectorstore = FAISS.from_documents(batch, self.embeddings)\n",
    "                    self.vector_store.merge_from(batch_vectorstore)\n",
    "            \n",
    "            self.logger.info(\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        í‚¤ì›Œë“œ ê¸°ë°˜ BM25 ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "            k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Document, float]]: (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        Raises:\n",
    "            ValueError: BM25ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°\n",
    "        \"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        self.logger.info(f\"'{query}' í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        top_k_idx = np.argsort(bm25_scores)[-k:][::-1]\n",
    "        results = [\n",
    "            (self.doc_mapping[idx], bm25_scores[idx])\n",
    "            for idx in top_k_idx\n",
    "        ]\n",
    "        \n",
    "        self.logger.info(f\"{len(results)}ê°œì˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(\n",
    "            self, \n",
    "            query: str, \n",
    "            k: int = 5, \n",
    "            semantic_weight: float = 0.5\n",
    "        ) -> List[Tuple[Document, float]]:\n",
    "            \"\"\"\n",
    "            ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "            \"\"\"\n",
    "            self.logger.info(f\"'{query}' í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìˆ˜í–‰\n",
    "            self.logger.info(f\"'{query}' ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "            semantic_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "            self.logger.info(f\"{len(semantic_results)}ê°œì˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "            keyword_results = self.keyword_search(query, k=k)\n",
    "            \n",
    "            # ë¬¸ì„œ IDë¥¼ í‚¤ë¡œ ì‚¬ìš©\n",
    "            combined_scores = {}\n",
    "            \n",
    "            # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬\n",
    "            max_semantic_score = max(score for _, score in semantic_results)\n",
    "            for doc, score in semantic_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                \n",
    "                #5ê°œì˜ ë¬¸ì„œì˜ ì ìˆ˜ê°€\n",
    "                normalized_score = 1 - (score / max_semantic_score) \n",
    "                combined_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'score': semantic_weight * normalized_score\n",
    "                }\n",
    "            \n",
    "            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬\n",
    "            max_keyword_score = max(score for _, score in keyword_results)\n",
    "            for doc, score in keyword_results:\n",
    "                doc_id = doc.metadata['chunk_id']\n",
    "                normalized_score = score / max_keyword_score\n",
    "                if doc_id in combined_scores:\n",
    "                    combined_scores[doc_id]['score'] += (1 - semantic_weight) * normalized_score\n",
    "                else:\n",
    "                    combined_scores[doc_id] = {\n",
    "                        'doc': doc,\n",
    "                        'score': (1 - semantic_weight) * normalized_score\n",
    "                    }\n",
    "            \n",
    "            # ê²°ê³¼ ì •ë ¬\n",
    "            sorted_results = sorted(\n",
    "                [(info['doc'], info['score']) for info in combined_scores.values()],\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "            \n",
    "            self.logger.info(f\"{len(sorted_results)}ê°œì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return sorted_results\n",
    "\n",
    "    def save_vector_store(self, vector_store_path: str, processed_docs_path:str=None):\n",
    "        \"\"\"\n",
    "        ë²¡í„° ìŠ¤í† ì–´ì™€ BM25 ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"ë°ì´í„°ë¥¼ {vector_store_path}ì— ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\n",
    "            os.makedirs(vector_store_path, exist_ok=True)\n",
    "            self.vector_store.save_local(vector_store_path)\n",
    "            \n",
    "            # processed_docs ì €ì¥\n",
    "            if self.processed_docs:\n",
    "                os.makedirs(os.path.dirname(processed_docs_path), exist_ok=True)\n",
    "                with open(processed_docs_path, 'wb') as f:\n",
    "                    pickle.dump(self.processed_docs, f)\n",
    "            \n",
    "            self.logger.info(\"ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_vector_store(self, vector_store_path: str, processed_docs_path):\n",
    "        \"\"\"\n",
    "        ë²¡í„° ìŠ¤í† ì–´ì™€ BM25 ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"ë°ì´í„°ë¥¼ {vector_store_path}ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "            \n",
    "            # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                vector_store_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # processed_docs ë¡œë“œ\n",
    "            if os.path.exists(processed_docs_path):\n",
    "                with open(processed_docs_path, 'rb') as f:\n",
    "                    self.processed_docs = pickle.load(f)\n",
    "                self.initialize_bm25(self.processed_docs)\n",
    "            \n",
    "            self.logger.info(\"ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIë‰´ìŠ¤ ë°ì´í„° Vector DB êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "print(\"ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ\n",
    "documents = rag.load_json_files(news_dir)\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "processed_docs = rag.process_documents(documents)\n",
    "rag.create_vector_store(processed_docs)\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\n",
    "rag.save_vector_store(vector_store_path, processed_doc_path)\n",
    "print(\"âœ… ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:38:56,843 - ë°ì´í„°ë¥¼ ai_news_vectorstoreì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:38:58,449 - BM25 ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:39:04,605 - BM25 ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "2024-11-28 11:39:04,676 - ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ” AI ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "- ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
      "\n",
      "'AI ë‰´ìŠ¤' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ëª¨ë“œ: hybrid)\n",
      "2024-11-28 11:39:08,417 - 'AI ë‰´ìŠ¤' í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:39:08,808 - 'AI ë‰´ìŠ¤' í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "2024-11-28 11:39:08,859 - 5ê°œì˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "2024-11-28 11:39:08,863 - 5ê°œì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ¨ ê²€ìƒ‰ ì™„ë£Œ! 5ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 1/5\n",
      "ì œëª©: ë„¤ì´ë²„, ë‰´ìŠ¤ AI ì•Œê³ ë¦¬ì¦˜ ì•ˆë‚´ í˜ì´ì§€ ê°•í™”\n",
      "ë‚ ì§œ: 2023.07.07 12:54\n",
      "í†µí•© ì ìˆ˜: 0.3000\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=152292\n",
      "----------------------------------------\n",
      "ë‚´ìš©:\n",
      "ë„¤ì´ë²„, ë‰´ìŠ¤ AI ì•Œê³ ë¦¬ì¦˜ ì•ˆë‚´ í˜ì´ì§€ ê°•í™”\n",
      " ë„¤ì´ë²„(ëŒ€í‘œ ìµœìˆ˜ì—°)ëŠ” ë‰´ìŠ¤ ì¸ê³µì§€ëŠ¥(AI) ì•Œê³ ë¦¬ì¦˜ ì•ˆë‚´ í˜ì´ì§€ë¥¼ êµ¬ì²´ì ì´ê³  ì´ìš©ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìƒˆë¡­ê²Œ ì„ ë³´ì˜€ë‹¤ê³  7ì¼ ë°í˜”ë‹¤.\n",
      "ë„¤ì´ë²„ ë‰´ìŠ¤ AI ì•Œê³ ë¦¬ì¦˜ ì•ˆë‚´ í˜ì´ì§€ëŠ” â–²ë„ì… ë°°ê²½ â–²AiRS(AI ì¶”ì²œ ì‹œìŠ¤í…œ) â–²ì•Œê³ ë¦¬ì¦˜ íŒ©í„° â–²í´ëŸ¬ìŠ¤í„°ë§ â–²ê¸°ìˆ  ê³ ë„í™” â–²FAQë¡œ êµ¬ì„±í–ˆë‹¤. ê¸°ì¡´ ì„œë¹„ìŠ¤ ì´ìš©ì•ˆë‚´-ì„œë¹„ìŠ¤ ìš´ì˜ì›ì¹™-ë‰´ìŠ¤ AI ì•Œê³ ë¦¬ì¦˜ê³¼ ê²€ìƒ‰ ë¸”ë¡œê·¸ ë“±ì— ê²Œì‹œí–ˆë˜ ê´€ë ¨ ë‚´ìš©ë³´ë‹¤ íˆ¬ëª…í•˜ê³  ì¼ëª©ìš”ì—°í•˜ê²Œ ë¶„ë¥˜í–ˆë‹¤ê³  ì„¤ëª…í–ˆë‹¤. íŠ¹íˆ ê°œì¸í™” ë° ë¹„ê°œì¸í™”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° í”¼ì²˜ë“¤ì˜ ì˜ë¯¸ë¥¼...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 2/5\n",
      "ì œëª©: í¬í‹°íˆ¬ë§ˆë£¨, ë¡œì´í„°ì™€ ë‰´ìŠ¤ ì½˜í…ì¸  ì œíœ´\n",
      "ë‚ ì§œ: 2024.10.18 14:05\n",
      "í†µí•© ì ìˆ˜: 0.2907\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=164365\n",
      "----------------------------------------\n",
      "ë‚´ìš©:\n",
      "í¬í‹°íˆ¬ë§ˆë£¨, ë¡œì´í„°ì™€ ë‰´ìŠ¤ ì½˜í…ì¸  ì œíœ´\n",
      " ìƒì„± ì¸ê³µì§€ëŠ¥(AI) ì „ë¬¸ í¬í‹°íˆ¬ë§ˆë£¨(ëŒ€í‘œ ê¹€ë™í™˜)ëŠ” ê¸€ë¡œë²Œ ë‰´ìŠ¤ ì œê³µ ì „ë¬¸ ë¡œì´í„°ì™€ ìƒì„± AI ì‹œëŒ€ ë‰´ ë¯¸ë””ì–´ ì„œë¹„ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ì„ ì²´ê²°í–ˆë‹¤ê³  18ì¼ ë°í˜”ë‹¤.\n",
      "ì´ë²ˆ íŒŒíŠ¸ë„ˆì‹­ìœ¼ë¡œ ë¡œì´í„° ì˜ìƒì„ í¬í•¨í•œ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ì½˜í…ì¸ ë¥¼ í¬í‹°íˆ¬ë§ˆë£¨ IT ì „ë¬¸ ë§¤ì²´ì—ì„œ í™œìš©í•  ìˆ˜ ìˆê²Œ ëë‹¤. í¬í‹°íˆ¬ë§ˆë£¨ëŠ” ë‰´ìŠ¤ ì½˜í…ì¸  ê¸°ë°˜ì˜ ë‰´ ë¯¸ë””ì–´ ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ë“± ì „ë°˜ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ íŒŒíŠ¸ë„ˆì‹­ì„ ì§„í–‰í•˜ê² ë‹¤ê³  ë°í˜”ë‹¤.\n",
      "ê¹€ë™í™˜ í¬í‹°íˆ¬ë§ˆë£¨ ëŒ€í‘œëŠ” â€œì´ë²ˆ í˜‘ë ¥ì„ ê³„ê¸°ë¡œ ë©€í‹°ëª¨ë‹¬ ê¸°ë°˜ ìƒì„± AI ì‹œëŒ€ì— ë¯¸ë””ì–´ ì‚°ì—…...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 3/5\n",
      "ì œëª©: êµ¬ê¸€ë„ ë‰´ìŠ¤ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ê³¨ë¨¸ë¦¬...\"ì—°ê°„ ìˆ˜ë°±ë§Œë‹¬ëŸ¬ëŠ” ì ì–´\"\n",
      "ë‚ ì§œ: 2024.05.01 17:10\n",
      "í†µí•© ì ìˆ˜: 0.2830\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=159264\n",
      "----------------------------------------\n",
      "ë‚´ìš©:\n",
      "êµ¬ê¸€ë„ ë‰´ìŠ¤ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ê³¨ë¨¸ë¦¬...\"ì—°ê°„ ìˆ˜ë°±ë§Œë‹¬ëŸ¬ëŠ” ì ì–´\"\n",
      " ì˜¤í”ˆAIê°€ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì¤„ì†Œì†¡ì„ ë‹¹í•˜ëŠ” ì‚¬ì´ì—, êµ¬ê¸€ì€ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ì €ë„(WSJ)ê³¼ ì—°ê°„ 500ë§Œ~600ë§Œë‹¬ëŸ¬(ì•½ 69ì–µ~83ì–µì›)ì—Â ë‹¬í•˜ëŠ” ê³„ì•½ì„ ë§ºì—ˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ì¸ê³µì§€ëŠ¥(AI) í•™ìŠµì— ê¸°ì‚¬ë¥¼ ì‚¬ìš©í•´ë„ ëœë‹¤ëŠ” ë¼ì´ì„ ìŠ¤ ê³„ì•½ì´ ì•„ë‹ˆë©°, ì €ì‘ê¶Œ ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì•„ì§ ì•¡ìˆ˜ ì°¨ê°€ í° ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤.\n",
      "ë”” ì¸í¬ë©”ì´ì…˜ì€ 30ì¼(í˜„ì§€ì‹œê°„) ê´€ê³„ìë¥¼ ì¸ìš©, WSJì„ ì†Œìœ í•œ ë‰´ìŠ¤ ì½”í¼ë ˆì´ì…˜(News Corp)ì´ êµ¬ê¸€ê³¼ ìƒˆë¡œìš´ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºì—ˆë‹¤ê³  ë³´ë„í–ˆë‹¤.\n",
      "ì´ì— ë”°ë¥´ë©´ ì´ë²ˆ ê³„ì•½...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 4/5\n",
      "ì œëª©: NC, ìì—°ì–´ ì¸ì‹í•˜ëŠ” ë¡œë´‡ ì†”ë£¨ì…˜ ê°œë°œ ë‚˜ì„°ë‹¤\n",
      "ë‚ ì§œ: 2023.09.12 16:42\n",
      "í†µí•© ì ìˆ˜: 0.2826\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=153584\n",
      "----------------------------------------\n",
      "ë‚´ìš©:\n",
      "NC ì—­ì‹œ êµ¬ê¸€ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¡œë´‡ ì œì–´ ì†”ë£¨ì…˜ ê°œë°œì— ë‚˜ì„  ê²ƒì´ë‹¤.\n",
      "í•œí¸ NCëŠ” 2011ë…„ë¶€í„° AI ì¡°ì§ì„ ìš´ì˜ ì¤‘ì´ë‹¤. í˜„ì¬ëŠ” ì•½ 300ì˜ ì „ë¬¸ ì¸ë ¥ì„ ê°–ì¶° AIì™€ NLPë“± ê³ ë„ì˜ ê¸°ìˆ ì„ ì—°êµ¬ê°œë°œí•˜ê³  ìˆë‹¤.\n",
      "ì´ë¯¸ í•­ê³µê¸°ìƒì²­ê³¼ ì—…ë¬´í˜‘ì•½ì„ ë§ºê³  AIÂ ê¸°ìˆ ì„ í™œìš©í•´ í•­ê³µ ê¸°ìƒì •ë³´ë¥¼ ì œì‘í•˜ê¸°ë¡œ í–ˆë‹¤. ë˜ ì°¨ëŸ‰ìš© ë‰´ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œÂ  'ëª¨ë¹Œë¦¬í‹° AIÂ ë‰´ìŠ¤ ì¸í”„ë¼ êµ¬ì¶•'ì—ë„ ë‚˜ì„°ë‹¤.\n",
      "ì¥ì„¸ë¯¼ ê¸°ì semim99@aitimes.com...\n",
      "\n",
      "================================================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼ 5/5\n",
      "ì œëª©: ë„¤ì´ë²„, ë‰´ìŠ¤ AI ì•Œê³ ë¦¬ì¦˜ ì•ˆë‚´ í˜ì´ì§€ ê°•í™”\n",
      "ë‚ ì§œ: 2023.07.07 12:54\n",
      "í†µí•© ì ìˆ˜: 0.2795\n",
      "URL: https://www.aitimes.com/news/articleView.html?idxno=152292\n",
      "----------------------------------------\n",
      "ë‚´ìš©:\n",
      "í•œí¸ 2019ë…„ë¶€í„° ìì²´ ê¸°ì‚¬ ë°°ì—´ì„ ì¤‘ë‹¨í•œ ë„¤ì´ë²„ëŠ” ì–¸ë¡ ì‚¬ê°€ ì§ì ‘ í¸ì§‘í•œ ê¸°ì‚¬ë¥¼ ì‚¬ìš©ìê°€ ì„ íƒí•˜ëŠ” êµ¬ë… ê¸°ë°˜ ì„œë¹„ìŠ¤ â€˜ì–¸ë¡ ì‚¬ í¸ì§‘â€™ê³¼ í•¨ê»˜ ë³´ì™„ì ìœ¼ë¡œ AI ì•Œê³ ë¦¬ì¦˜ì— ê¸°ë°˜í•œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µ ì¤‘ì´ë‹¤. AiRS ê°œì¸í™” ì¶”ì²œ, ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ ë“±ì˜ AI ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë°”ì¼ MYë‰´ìŠ¤, ëª¨ë°”ì¼ ë° PC ì„¹ì…˜ë³„ ë‰´ìŠ¤ ì˜ì—­ ë“±ì—ì„œ ì‘ë™ ì¤‘ì´ë‹¤.\n",
      "ì´ì£¼ì˜ ê¸°ì juyoung09@aitimes.com...\n",
      "\n",
      "ğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "vector_store_path = os.getenv(\"VECTOR_STORE_NAME\", \"ai_news_vectorstore\")\n",
    "news_dir = os.getenv(\"NEWS_FILE_PATH\", \"./ai_news\")\n",
    "processed_doc_path = os.getenv(\"PROCESSED_DOCS_PATH\", \"processed_docs/processed_docs.pkl\")\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” \n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = AINewsRAG(embedding_model)\n",
    "\n",
    "try:\n",
    "    # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„\n",
    "    rag.load_vector_store(vector_store_path, processed_doc_path)\n",
    "    print(\"âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œì‘\n",
    "print(\"\\nğŸ” AI ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "print(\"- ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "search_mode = \"hybrid\" # ê²€ìƒ‰ ë°©ì‹ ë³€ê²½ì€ 'mode [semantic/keyword/hybrid]'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
    "while True:\n",
    "    query = input(\"\\nğŸ” ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "        \n",
    "    if query.lower() in ['q', 'quit']:\n",
    "        print(\"\\nğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "        \n",
    "    if query.lower().startswith('mode '):\n",
    "        mode = query.split()[1].lower()\n",
    "        if mode in ['semantic', 'keyword', 'hybrid']:\n",
    "            search_mode = mode\n",
    "            print(f\"\\nâœ… ê²€ìƒ‰ ëª¨ë“œë¥¼ '{mode}'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"\\nâŒ ì˜ëª»ëœ ê²€ìƒ‰ ëª¨ë“œì…ë‹ˆë‹¤. semantic/keyword/hybrid ì¤‘ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n'{query}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ëª¨ë“œ: {search_mode})\")\n",
    "        \n",
    "        if search_mode == \"hybrid\":\n",
    "            results = rag.hybrid_search(query, k=5, semantic_weight=0.5)\n",
    "        elif search_mode == \"semantic\":\n",
    "            results = rag.vector_store.similarity_search_with_score(query, k=5)\n",
    "        else:  # keyword\n",
    "            results = rag.keyword_search(query, k=5)\n",
    "        \n",
    "        print(f\"\\nâœ¨ ê²€ìƒ‰ ì™„ë£Œ! {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\\n\")\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ê²€ìƒ‰ ê²°ê³¼ {i}/{len(results)}\")\n",
    "            print(f\"ì œëª©: {doc.metadata['title']}\")\n",
    "            print(f\"ë‚ ì§œ: {doc.metadata['date']}\")\n",
    "            if search_mode == \"hybrid\":\n",
    "                print(f\"í†µí•© ì ìˆ˜: {score:.4f}\")\n",
    "            elif search_mode == \"semantic\":\n",
    "                print(f\"ìœ ì‚¬ë„ ì ìˆ˜: {1 - (score/2):.4f}\")\n",
    "            else:\n",
    "                print(f\"BM25 ì ìˆ˜: {score:.4f}\")\n",
    "            print(f\"URL: {doc.metadata['url']}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            print(f\"ë‚´ìš©:\\n{doc.page_content[:300]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
